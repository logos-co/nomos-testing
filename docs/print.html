<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Nomos Testing Book</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="project-context-primer.html"><strong aria-hidden="true">1.</strong> Project Context Primer</a></li><li class="chapter-item expanded "><a href="what-you-will-learn.html"><strong aria-hidden="true">2.</strong> What You Will Learn</a></li><li class="chapter-item expanded "><a href="part-i.html"><strong aria-hidden="true">3.</strong> Part I — Foundations</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="introduction.html"><strong aria-hidden="true">3.1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="architecture-overview.html"><strong aria-hidden="true">3.2.</strong> Architecture Overview</a></li><li class="chapter-item expanded "><a href="testing-philosophy.html"><strong aria-hidden="true">3.3.</strong> Testing Philosophy</a></li><li class="chapter-item expanded "><a href="scenario-lifecycle.html"><strong aria-hidden="true">3.4.</strong> Scenario Lifecycle</a></li><li class="chapter-item expanded "><a href="design-rationale.html"><strong aria-hidden="true">3.5.</strong> Design Rationale</a></li></ol></li><li class="chapter-item expanded "><a href="part-ii.html"><strong aria-hidden="true">4.</strong> Part II — User Guide</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="workspace-layout.html"><strong aria-hidden="true">4.1.</strong> Workspace Layout</a></li><li class="chapter-item expanded "><a href="annotated-tree.html"><strong aria-hidden="true">4.2.</strong> Annotated Tree</a></li><li class="chapter-item expanded "><a href="authoring-scenarios.html"><strong aria-hidden="true">4.3.</strong> Authoring Scenarios</a></li><li class="chapter-item expanded "><a href="workloads.html"><strong aria-hidden="true">4.4.</strong> Core Content: Workloads & Expectations</a></li><li class="chapter-item expanded "><a href="scenario-builder-ext-patterns.html"><strong aria-hidden="true">4.5.</strong> Core Content: ScenarioBuilderExt Patterns</a></li><li class="chapter-item expanded "><a href="best-practices.html"><strong aria-hidden="true">4.6.</strong> Best Practices</a></li><li class="chapter-item expanded "><a href="examples.html"><strong aria-hidden="true">4.7.</strong> Examples</a></li><li class="chapter-item expanded "><a href="examples-advanced.html"><strong aria-hidden="true">4.8.</strong> Advanced & Artificial Examples</a></li><li class="chapter-item expanded "><a href="running-scenarios.html"><strong aria-hidden="true">4.9.</strong> Running Scenarios</a></li><li class="chapter-item expanded "><a href="runners.html"><strong aria-hidden="true">4.10.</strong> Runners</a></li><li class="chapter-item expanded "><a href="operations.html"><strong aria-hidden="true">4.11.</strong> Operations</a></li></ol></li><li class="chapter-item expanded "><a href="part-iii.html"><strong aria-hidden="true">5.</strong> Part III — Developer Reference</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="scenario-model.html"><strong aria-hidden="true">5.1.</strong> Scenario Model (Developer Level)</a></li><li class="chapter-item expanded "><a href="extending.html"><strong aria-hidden="true">5.2.</strong> Extending the Framework</a></li><li class="chapter-item expanded "><a href="custom-workload-example.html"><strong aria-hidden="true">5.3.</strong> Example: New Workload & Expectation (Rust)</a></li><li class="chapter-item expanded "><a href="internal-crate-reference.html"><strong aria-hidden="true">5.4.</strong> Internal Crate Reference</a></li></ol></li><li class="chapter-item expanded "><a href="part-iv.html"><strong aria-hidden="true">6.</strong> Part IV — Appendix</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dsl-cheat-sheet.html"><strong aria-hidden="true">6.1.</strong> Builder API Quick Reference</a></li><li class="chapter-item expanded "><a href="troubleshooting.html"><strong aria-hidden="true">6.2.</strong> Troubleshooting Scenarios</a></li><li class="chapter-item expanded "><a href="faq.html"><strong aria-hidden="true">6.3.</strong> FAQ</a></li><li class="chapter-item expanded "><a href="glossary.html"><strong aria-hidden="true">6.4.</strong> Glossary</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Nomos Testing Book</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="project-context-primer"><a class="header" href="#project-context-primer">Project Context Primer</a></h1>
<p>This book focuses on the Nomos Testing Framework. It assumes familiarity with
the Nomos architecture, but for completeness, here is a short primer.</p>
<ul>
<li><strong>Nomos</strong> is a modular blockchain protocol composed of validators, executors,
and a data-availability (DA) subsystem.</li>
<li><strong>Validators</strong> participate in consensus and produce blocks.</li>
<li><strong>Executors</strong> are validators with the DA dispersal service enabled. They perform
all validator functions plus submit blob data to the DA network.</li>
<li><strong>Data Availability (DA)</strong> ensures that blob data submitted via channel operations
in transactions is published and retrievable by the network.</li>
</ul>
<p>These roles interact tightly, which is why meaningful testing must be performed
in multi-node environments that include real networking, timing, and DA
interaction.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="what-you-will-learn"><a class="header" href="#what-you-will-learn">What You Will Learn</a></h1>
<p>This book gives you a clear mental model for Nomos multi-node testing, shows how
to author scenarios that pair realistic workloads with explicit expectations,
and guides you to run them across local, containerized, and cluster environments
without changing the plan.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="part-i--foundations"><a class="header" href="#part-i--foundations">Part I — Foundations</a></h1>
<p>Conceptual chapters that establish the mental model for the framework and how
it approaches multi-node testing.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>The Nomos Testing Framework is a purpose-built toolkit for exercising Nomos in
realistic, multi-node environments. It solves the gap between small, isolated
tests and full-system validation by letting teams describe a cluster layout,
drive meaningful traffic, and assert the outcomes in one coherent plan.</p>
<p>It is for protocol engineers, infrastructure operators, and QA teams who need
repeatable confidence that validators, executors, and data-availability
components work together under network and timing constraints.</p>
<p>Multi-node integration testing is required because many Nomos behaviors—block
progress, data availability, liveness under churn—only emerge when several
roles interact over real networking and time. This framework makes those checks
declarative, observable, and portable across environments.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="architecture-overview"><a class="header" href="#architecture-overview">Architecture Overview</a></h1>
<p>The framework follows a clear flow: <strong>Topology → Scenario → Deployer → Runner → Workloads → Expectations</strong>.</p>
<h2 id="core-flow"><a class="header" href="#core-flow">Core Flow</a></h2>
<pre><code class="language-mermaid">flowchart LR
    A(Topology&lt;br/&gt;shape cluster) --&gt; B(Scenario&lt;br/&gt;plan)
    B --&gt; C(Deployer&lt;br/&gt;provision &amp; readiness)
    C --&gt; D(Runner&lt;br/&gt;orchestrate execution)
    D --&gt; E(Workloads&lt;br/&gt;drive traffic)
    E --&gt; F(Expectations&lt;br/&gt;verify outcomes)
</code></pre>
<h3 id="components"><a class="header" href="#components">Components</a></h3>
<ul>
<li><strong>Topology</strong> describes the cluster: how many nodes, their roles, and the high-level network and data-availability parameters they should follow.</li>
<li><strong>Scenario</strong> combines that topology with the activities to run and the checks to perform, forming a single plan.</li>
<li><strong>Deployer</strong> provisions infrastructure on the chosen backend (local processes, Docker Compose, or Kubernetes), waits for readiness, and returns a Runner.</li>
<li><strong>Runner</strong> orchestrates scenario execution: starts workloads, observes signals, evaluates expectations, and triggers cleanup.</li>
<li><strong>Workloads</strong> generate traffic and conditions that exercise the system.</li>
<li><strong>Expectations</strong> observe the run and judge success or failure once activity completes.</li>
</ul>
<p>Each layer has a narrow responsibility so that cluster shape, deployment choice,
traffic generation, and health checks can evolve independently while fitting
together predictably.</p>
<h2 id="entry-points"><a class="header" href="#entry-points">Entry Points</a></h2>
<p>The framework is consumed via <strong>runnable example binaries</strong> in <code>examples/src/bin/</code>:</p>
<ul>
<li><code>local_runner.rs</code> — Spawns nodes as host processes</li>
<li><code>compose_runner.rs</code> — Deploys via Docker Compose (requires <code>NOMOS_TESTNET_IMAGE</code> built)</li>
<li><code>k8s_runner.rs</code> — Deploys via Kubernetes Helm (requires cluster + image)</li>
</ul>
<p><strong>Recommended:</strong> Use the convenience script:</p>
<pre><code class="language-bash">scripts/run-examples.sh -t &lt;duration&gt; -v &lt;validators&gt; -e &lt;executors&gt; &lt;mode&gt;
# mode: host, compose, or k8s
</code></pre>
<p>This handles circuit setup, binary building/bundling, image building, and execution.</p>
<p><strong>Alternative:</strong> Direct cargo run (requires manual setup):</p>
<pre><code class="language-bash">POL_PROOF_DEV_MODE=true cargo run -p runner-examples --bin &lt;name&gt;
</code></pre>
<p><strong>Important:</strong> All runners require <code>POL_PROOF_DEV_MODE=true</code> to avoid expensive Groth16 proof generation that causes timeouts.</p>
<p>These binaries use the framework API (<code>ScenarioBuilder</code>) to construct and execute scenarios.</p>
<h2 id="builder-api"><a class="header" href="#builder-api">Builder API</a></h2>
<p>Scenarios are defined using a fluent builder pattern:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut plan = ScenarioBuilder::topology_with(|t| {
        t.network_star()      // Topology configuration
            .validators(3)
            .executors(2)
    })
    .wallets(50)             // Wallet seeding
    .transactions_with(|txs| {
        txs.rate(5)
            .users(20)
    })
    .da_with(|da| {
        da.channel_rate(1)
            .blob_rate(2)
    })
    .expect_consensus_liveness()  // Expectations
    .with_run_duration(Duration::from_secs(90))
    .build();
<span class="boring">}</span></code></pre></pre>
<p><strong>Key API Points:</strong></p>
<ul>
<li>Topology uses <code>.topology_with(|t| { t.validators(N).executors(M) })</code> closure pattern</li>
<li>Workloads are configured via <code>_with</code> closures (<code>transactions_with</code>, <code>da_with</code>, <code>chaos_with</code>)</li>
<li>Chaos workloads require <code>.enable_node_control()</code> and a compatible runner</li>
</ul>
<h2 id="deployers"><a class="header" href="#deployers">Deployers</a></h2>
<p>Three deployer implementations:</p>
<div class="table-wrapper"><table><thead><tr><th>Deployer</th><th>Backend</th><th>Prerequisites</th><th>Node Control</th></tr></thead><tbody>
<tr><td><code>LocalDeployer</code></td><td>Host processes</td><td>Binaries (built on demand or via bundle)</td><td>No</td></tr>
<tr><td><code>ComposeDeployer</code></td><td>Docker Compose</td><td>Image with embedded assets/binaries</td><td>Yes</td></tr>
<tr><td><code>K8sDeployer</code></td><td>Kubernetes Helm</td><td>Cluster + image loaded</td><td>Not yet</td></tr>
</tbody></table>
</div>
<p><strong>Compose-specific features:</strong></p>
<ul>
<li>Includes Prometheus at <code>http://localhost:9090</code> (override via <code>TEST_FRAMEWORK_PROMETHEUS_PORT</code>)</li>
<li>Optional OTLP trace/metrics endpoints (<code>NOMOS_OTLP_ENDPOINT</code>, <code>NOMOS_OTLP_METRICS_ENDPOINT</code>)</li>
<li>Node control for chaos testing (restart validators/executors)</li>
</ul>
<h2 id="assets-and-images"><a class="header" href="#assets-and-images">Assets and Images</a></h2>
<h3 id="docker-image"><a class="header" href="#docker-image">Docker Image</a></h3>
<p>Built via <code>testing-framework/assets/stack/scripts/build_test_image.sh</code>:</p>
<ul>
<li>Embeds KZG circuit parameters and binaries from <code>testing-framework/assets/stack/kzgrs_test_params/kzgrs_test_params</code></li>
<li>Includes runner scripts: <code>run_nomos_node.sh</code>, <code>run_nomos_executor.sh</code></li>
<li>Tagged as <code>NOMOS_TESTNET_IMAGE</code> (default: <code>nomos-testnet:local</code>)</li>
<li><strong>Recommended:</strong> Use prebuilt bundle via <code>scripts/build-bundle.sh --platform linux</code> and set <code>NOMOS_BINARIES_TAR</code> before building image</li>
</ul>
<h3 id="circuit-assets"><a class="header" href="#circuit-assets">Circuit Assets</a></h3>
<p>KZG parameters required for DA workloads:</p>
<ul>
<li><strong>Host path:</strong> <code>testing-framework/assets/stack/kzgrs_test_params/kzgrs_test_params</code> (note repeated filename—directory contains file <code>kzgrs_test_params</code>)</li>
<li><strong>Container path:</strong> <code>/kzgrs_test_params/kzgrs_test_params</code> (for compose/k8s)</li>
<li><strong>Override:</strong> <code>NOMOS_KZGRS_PARAMS_PATH=/custom/path/to/file</code> (must point to file)</li>
<li><strong>Fetch via:</strong> <code>scripts/setup-nomos-circuits.sh v0.3.1 /tmp/circuits</code> or use <code>scripts/run-examples.sh</code></li>
</ul>
<h3 id="compose-stack"><a class="header" href="#compose-stack">Compose Stack</a></h3>
<p>Templates and configs in <code>testing-framework/runners/compose/assets/</code>:</p>
<ul>
<li><code>docker-compose.yml.tera</code> — Stack template (validators, executors, Prometheus)</li>
<li>Cfgsync config: <code>testing-framework/assets/stack/cfgsync.yaml</code></li>
<li>Monitoring: <code>testing-framework/assets/stack/monitoring/prometheus.yml</code></li>
</ul>
<h2 id="logging-architecture"><a class="header" href="#logging-architecture">Logging Architecture</a></h2>
<p><strong>Two separate logging pipelines:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Component</th><th>Configuration</th><th>Output</th></tr></thead><tbody>
<tr><td><strong>Runner binaries</strong></td><td><code>RUST_LOG</code></td><td>Framework orchestration logs</td></tr>
<tr><td><strong>Node processes</strong></td><td><code>NOMOS_LOG_LEVEL</code>, <code>NOMOS_LOG_FILTER</code>, <code>NOMOS_LOG_DIR</code></td><td>Consensus, DA, mempool logs</td></tr>
</tbody></table>
</div>
<p><strong>Node logging:</strong></p>
<ul>
<li><strong>Local runner:</strong> Writes to temporary directories by default (cleaned up). Set <code>NOMOS_TESTS_TRACING=true</code> + <code>NOMOS_LOG_DIR</code> for persistent files.</li>
<li><strong>Compose runner:</strong> Default logs to container stdout/stderr (<code>docker logs</code>). Optional per-node files if <code>NOMOS_LOG_DIR</code> is set and mounted.</li>
<li><strong>K8s runner:</strong> Logs to pod stdout/stderr (<code>kubectl logs</code>). Optional per-node files if <code>NOMOS_LOG_DIR</code> is set and mounted.</li>
</ul>
<p><strong>File naming:</strong> Per-node files use prefix <code>nomos-node-{index}</code> or <code>nomos-executor-{index}</code> (may include timestamps).</p>
<h2 id="observability"><a class="header" href="#observability">Observability</a></h2>
<p><strong>Prometheus (Compose only):</strong></p>
<ul>
<li>Exposed at <code>http://localhost:9090</code> (configurable)</li>
<li>Scrapes all validator and executor metrics</li>
<li>Accessible in expectations: <code>ctx.telemetry().prometheus_endpoint()</code></li>
</ul>
<p><strong>Node APIs:</strong></p>
<ul>
<li>HTTP endpoints per node for consensus info, network status, DA membership</li>
<li>Accessible in expectations: <code>ctx.node_clients().validators().get(0)</code></li>
</ul>
<p><strong>OTLP (optional):</strong></p>
<ul>
<li>Trace endpoint: <code>NOMOS_OTLP_ENDPOINT=http://localhost:4317</code></li>
<li>Metrics endpoint: <code>NOMOS_OTLP_METRICS_ENDPOINT=http://localhost:4318</code></li>
<li>Disabled by default (no noise if unset)</li>
</ul>
<p>For detailed logging configuration, see <a href="operations.html#logging-and-observability">Logging and Observability</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="testing-philosophy"><a class="header" href="#testing-philosophy">Testing Philosophy</a></h1>
<p>This framework embodies specific principles that shape how you author and run
scenarios. Understanding these principles helps you write effective tests and
interpret results correctly.</p>
<h2 id="declarative-over-imperative"><a class="header" href="#declarative-over-imperative">Declarative over Imperative</a></h2>
<p>Describe <strong>what</strong> you want to test, not <strong>how</strong> to orchestrate it:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Good: declarative
ScenarioBuilder::topology_with(|t| {
        t.network_star()
            .validators(2)
            .executors(1)
    })
    .transactions_with(|txs| {
        txs.rate(5)             // 5 transactions per block
    })
    .expect_consensus_liveness()
    .build();

// Bad: imperative (framework doesn't work this way)
// spawn_validator(); spawn_executor(); 
// loop { submit_tx(); check_block(); }
<span class="boring">}</span></code></pre></pre>
<p><strong>Why it matters:</strong> The framework handles deployment, readiness, and cleanup.
You focus on test intent, not infrastructure orchestration.</p>
<h2 id="protocol-time-not-wall-time"><a class="header" href="#protocol-time-not-wall-time">Protocol Time, Not Wall Time</a></h2>
<p>Reason in <strong>blocks</strong> and <strong>consensus intervals</strong>, not wall-clock seconds.</p>
<p><strong>Consensus defaults:</strong></p>
<ul>
<li>Slot duration: 2 seconds (NTP-synchronized, configurable via <code>CONSENSUS_SLOT_TIME</code>)</li>
<li>Active slot coefficient: 0.9 (90% block probability per slot)</li>
<li>Expected rate: ~27 blocks per minute</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Good: protocol-oriented thinking
let plan = ScenarioBuilder::topology_with(|t| {
        t.network_star()
            .validators(2)
            .executors(1)
    })
    .transactions_with(|txs| {
        txs.rate(5)             // 5 transactions per block
    })
    .with_run_duration(Duration::from_secs(60))  // Let framework calculate expected blocks
    .expect_consensus_liveness()  // "Did we produce the expected blocks?"
    .build();

// Bad: wall-clock assumptions
// "I expect exactly 30 blocks in 60 seconds"
// This breaks on slow CI where slot timing might drift
<span class="boring">}</span></code></pre></pre>
<p><strong>Why it matters:</strong> Slot timing is fixed (2s by default, NTP-synchronized), so the
expected number of blocks is predictable: ~27 blocks in 60s with the default
0.9 active slot coefficient. The framework calculates expected blocks from slot
duration and run window, making assertions protocol-based rather than tied to
specific wall-clock expectations. Assert on "blocks produced relative to slots"
not "blocks produced in exact wall-clock seconds".</p>
<h2 id="determinism-first-chaos-when-needed"><a class="header" href="#determinism-first-chaos-when-needed">Determinism First, Chaos When Needed</a></h2>
<p><strong>Default scenarios are repeatable:</strong></p>
<ul>
<li>Fixed topology</li>
<li>Predictable traffic rates</li>
<li>Deterministic checks</li>
</ul>
<p><strong>Chaos is opt-in:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Separate: functional test (deterministic)
let plan = ScenarioBuilder::topology_with(|t| {
        t.network_star()
            .validators(2)
            .executors(1)
    })
    .transactions_with(|txs| {
        txs.rate(5)             // 5 transactions per block
    })
    .expect_consensus_liveness()
    .build();

// Separate: chaos test (introduces randomness)
let chaos_plan = ScenarioBuilder::topology_with(|t| {
        t.network_star()
            .validators(3)
            .executors(2)
    })
    .enable_node_control()
    .chaos_with(|c| {
        c.restart()
            .min_delay(Duration::from_secs(30))
            .max_delay(Duration::from_secs(60))
            .target_cooldown(Duration::from_secs(45))
            .apply()
    })
    .transactions_with(|txs| {
        txs.rate(5)             // 5 transactions per block
    })
    .expect_consensus_liveness()
    .build();
<span class="boring">}</span></code></pre></pre>
<p><strong>Why it matters:</strong> Mixing determinism with chaos creates noisy, hard-to-debug
failures. Separate concerns make failures actionable.</p>
<h2 id="observable-health-signals"><a class="header" href="#observable-health-signals">Observable Health Signals</a></h2>
<p>Prefer <strong>user-facing signals</strong> over internal state:</p>
<p><strong>Good checks:</strong></p>
<ul>
<li>Blocks progressing at expected rate (liveness)</li>
<li>Transactions included within N blocks (inclusion)</li>
<li>DA blobs retrievable (availability)</li>
</ul>
<p><strong>Avoid internal checks:</strong></p>
<ul>
<li>Memory pool size</li>
<li>Internal service state</li>
<li>Cache hit rates</li>
</ul>
<p><strong>Why it matters:</strong> User-facing signals reflect actual system health.
Internal state can be "healthy" while the system is broken from a user
perspective.</p>
<h2 id="minimum-run-windows"><a class="header" href="#minimum-run-windows">Minimum Run Windows</a></h2>
<p>Always run long enough for <strong>meaningful block production</strong>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Bad: too short
.with_run_duration(Duration::from_secs(5))  // ~2 blocks (with default 2s slots, 0.9 coeff)

// Good: enough blocks for assertions
.with_run_duration(Duration::from_secs(60))  // ~27 blocks (with default 2s slots, 0.9 coeff)
<span class="boring">}</span></code></pre></pre>
<p><strong>Note:</strong> Block counts assume default consensus parameters:</p>
<ul>
<li>Slot duration: 2 seconds (configurable via <code>CONSENSUS_SLOT_TIME</code>)</li>
<li>Active slot coefficient: 0.9 (90% block probability per slot)</li>
<li>Formula: <code>blocks ≈ (duration / slot_duration) × active_slot_coeff</code></li>
</ul>
<p>If upstream changes these parameters, adjust your duration expectations accordingly.</p>
<p>The framework enforces minimum durations (at least 2× slot duration), but be explicit. Very short runs risk false confidence—one lucky block doesn't prove liveness.</p>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>These principles keep scenarios:</p>
<ul>
<li><strong>Portable</strong> across environments (protocol time, declarative)</li>
<li><strong>Debuggable</strong> (determinism, separation of concerns)</li>
<li><strong>Meaningful</strong> (observable signals, sufficient duration)</li>
</ul>
<p>When authoring scenarios, ask: "Does this test the protocol behavior or
my local environment quirks?"</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scenario-lifecycle"><a class="header" href="#scenario-lifecycle">Scenario Lifecycle</a></h1>
<ol>
<li><strong>Build the plan</strong>: Declare a topology, attach workloads and expectations, and set the run window. The plan is the single source of truth for what will happen.</li>
<li><strong>Deploy</strong>: Hand the plan to a deployer. It provisions the environment on the chosen backend, waits for nodes to signal readiness, and returns a runner.</li>
<li><strong>Drive workloads</strong>: The runner starts traffic and behaviors (transactions, data-availability activity, restarts) for the planned duration.</li>
<li><strong>Observe blocks and signals</strong>: Track block progression and other high-level metrics during or after the run window to ground assertions in protocol time.</li>
<li><strong>Evaluate expectations</strong>: Once activity stops (and optional cooldown completes), the runner checks liveness and workload-specific outcomes to decide pass or fail.</li>
<li><strong>Cleanup</strong>: Tear down resources so successive runs start fresh and do not inherit leaked state.</li>
</ol>
<pre><code class="language-mermaid">flowchart LR
    P[Plan&lt;br/&gt;topology + workloads + expectations] --&gt; D[Deploy&lt;br/&gt;deployer provisions]
    D --&gt; R[Runner&lt;br/&gt;orchestrates execution]
    R --&gt; W[Drive Workloads]
    W --&gt; O[Observe&lt;br/&gt;blocks/metrics]
    O --&gt; E[Evaluate Expectations]
    E --&gt; C[Cleanup]
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="design-rationale"><a class="header" href="#design-rationale">Design Rationale</a></h1>
<ul>
<li><strong>Modular crates</strong> keep configuration, orchestration, workloads, and runners decoupled so each can evolve without breaking the others.</li>
<li><strong>Pluggable runners</strong> let the same scenario run on a laptop, a Docker host, or a Kubernetes cluster, making validation portable across environments.</li>
<li><strong>Separated workloads and expectations</strong> clarify intent: what traffic to generate versus how to judge success. This simplifies review and reuse.</li>
<li><strong>Declarative topology</strong> makes cluster shape explicit and repeatable, reducing surprise when moving between CI and developer machines.</li>
<li><strong>Maintainability through predictability</strong>: a clear flow from plan to deployment to verification lowers the cost of extending the framework and interpreting failures.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="part-ii--user-guide"><a class="header" href="#part-ii--user-guide">Part II — User Guide</a></h1>
<p>Practical guidance for shaping scenarios, combining workloads and expectations,
and running them across different environments.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="workspace-layout"><a class="header" href="#workspace-layout">Workspace Layout</a></h1>
<p>The workspace focuses on multi-node integration testing and sits alongside a
<code>nomos-node</code> checkout. Its crates separate concerns to keep scenarios
repeatable and portable:</p>
<ul>
<li><strong>Configs</strong>: prepares high-level node, network, tracing, and wallet settings
used across test environments.</li>
<li><strong>Core scenario orchestration</strong>: the engine that holds topology descriptions,
scenario plans, runtimes, workloads, and expectations.</li>
<li><strong>Workflows</strong>: ready-made workloads (transactions, data-availability, chaos)
and reusable expectations assembled into a user-facing DSL.</li>
<li><strong>Runners</strong>: deployment backends for local processes, Docker Compose, and
Kubernetes, all consuming the same scenario plan.</li>
<li><strong>Runner Examples</strong> (<code>examples/runner-examples</code>): runnable binaries
(<code>local_runner.rs</code>, <code>compose_runner.rs</code>, <code>k8s_runner.rs</code>) that demonstrate
complete scenario execution with each deployer.</li>
</ul>
<p>This split keeps configuration, orchestration, reusable traffic patterns, and
deployment adapters loosely coupled while sharing one mental model for tests.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="annotated-tree"><a class="header" href="#annotated-tree">Annotated Tree</a></h1>
<p>Directory structure with key paths annotated:</p>
<pre><code>nomos-testing/
├─ testing-framework/           # Core library crates
│  ├─ configs/                  # Node config builders, topology generation, tracing/logging config
│  ├─ core/                     # Scenario model (ScenarioBuilder), runtime (Runner, Deployer), topology, node spawning
│  ├─ workflows/                # Workloads (transactions, DA, chaos), expectations (liveness), builder DSL extensions
│  ├─ runners/                  # Deployment backends
│  │  ├─ local/                 # LocalDeployer (spawns local processes)
│  │  ├─ compose/               # ComposeDeployer (Docker Compose + Prometheus)
│  │  └─ k8s/                   # K8sDeployer (Kubernetes Helm)
│  └─ assets/                   # Docker/K8s stack assets
│     └─ stack/
│        ├─ kzgrs_test_params/  # KZG circuit parameters directory
│        │  └─ kzgrs_test_params  # Actual proving key file (note repeated name)
│        ├─ monitoring/         # Prometheus config
│        ├─ scripts/            # Container entrypoints, image builder
│        └─ cfgsync.yaml        # Config sync server template
│
├─ examples/                    # PRIMARY ENTRY POINT: runnable binaries
│  └─ src/bin/
│     ├─ local_runner.rs        # Host processes demo (LocalDeployer)
│     ├─ compose_runner.rs      # Docker Compose demo (ComposeDeployer)
│     └─ k8s_runner.rs          # Kubernetes demo (K8sDeployer)
│
├─ scripts/                     # Helper utilities
│  ├─ run-examples.sh           # Convenience script (handles setup + runs examples)
│  ├─ build-bundle.sh           # Build prebuilt binaries+circuits bundle
│  ├─ setup-circuits-stack.sh  # Fetch KZG parameters (Linux + host)
│  └─ setup-nomos-circuits.sh  # Legacy circuit fetcher
│
└─ book/                        # This documentation (mdBook)
</code></pre>
<h2 id="key-directories-explained"><a class="header" href="#key-directories-explained">Key Directories Explained</a></h2>
<h3 id="testing-framework"><a class="header" href="#testing-framework"><code>testing-framework/</code></a></h3>
<p>Core library crates providing the testing API.</p>
<div class="table-wrapper"><table><thead><tr><th>Crate</th><th>Purpose</th><th>Key Exports</th></tr></thead><tbody>
<tr><td><code>configs</code></td><td>Node configuration builders</td><td>Topology generation, tracing config</td></tr>
<tr><td><code>core</code></td><td>Scenario model &amp; runtime</td><td><code>ScenarioBuilder</code>, <code>Deployer</code>, <code>Runner</code></td></tr>
<tr><td><code>workflows</code></td><td>Workloads &amp; expectations</td><td><code>ScenarioBuilderExt</code>, <code>ChaosBuilderExt</code></td></tr>
<tr><td><code>runners/local</code></td><td>Local process deployer</td><td><code>LocalDeployer</code></td></tr>
<tr><td><code>runners/compose</code></td><td>Docker Compose deployer</td><td><code>ComposeDeployer</code></td></tr>
<tr><td><code>runners/k8s</code></td><td>Kubernetes deployer</td><td><code>K8sDeployer</code></td></tr>
</tbody></table>
</div>
<h3 id="testing-frameworkassetsstack"><a class="header" href="#testing-frameworkassetsstack"><code>testing-framework/assets/stack/</code></a></h3>
<p>Docker/K8s deployment assets:</p>
<ul>
<li><strong><code>kzgrs_test_params/kzgrs_test_params</code></strong>: Circuit parameters file (note repeated name; override via <code>NOMOS_KZGRS_PARAMS_PATH</code>)</li>
<li><strong><code>monitoring/</code></strong>: Prometheus config</li>
<li><strong><code>scripts/</code></strong>: Container entrypoints and image builder</li>
</ul>
<h3 id="scripts"><a class="header" href="#scripts"><code>scripts/</code></a></h3>
<p>Convenience utilities:</p>
<ul>
<li><strong><code>run-examples.sh</code></strong>: All-in-one script for host/compose/k8s modes (recommended)</li>
<li><strong><code>build-bundle.sh</code></strong>: Create prebuilt binaries+circuits bundle for compose/k8s</li>
<li><strong><code>setup-circuits-stack.sh</code></strong>: Fetch KZG parameters for both Linux and host</li>
<li><strong><code>cfgsync.yaml</code></strong>: Configuration sync server template</li>
</ul>
<h3 id="examples-start-here"><a class="header" href="#examples-start-here"><code>examples/</code> (Start Here!)</a></h3>
<p><strong>Runnable binaries</strong> demonstrating framework usage:</p>
<ul>
<li><code>local_runner.rs</code> — Local processes</li>
<li><code>compose_runner.rs</code> — Docker Compose (requires <code>NOMOS_TESTNET_IMAGE</code> built)</li>
<li><code>k8s_runner.rs</code> — Kubernetes (requires cluster + image)</li>
</ul>
<p><strong>Run with:</strong> <code>POL_PROOF_DEV_MODE=true cargo run -p runner-examples --bin &lt;name&gt;</code></p>
<p><strong>All runners require <code>POL_PROOF_DEV_MODE=true</code></strong> to avoid expensive proof generation.</p>
<h3 id="scripts-1"><a class="header" href="#scripts-1"><code>scripts/</code></a></h3>
<p>Helper utilities:</p>
<ul>
<li><strong><code>setup-nomos-circuits.sh</code></strong>: Fetch KZG parameters from releases</li>
</ul>
<h2 id="observability-1"><a class="header" href="#observability-1">Observability</a></h2>
<p><strong>Compose runner</strong> includes:</p>
<ul>
<li><strong>Prometheus</strong> at <code>http://localhost:9090</code> (metrics scraping)</li>
<li>Node metrics exposed per validator/executor</li>
<li>Access in expectations: <code>ctx.telemetry().prometheus_endpoint()</code></li>
</ul>
<p><strong>Logging</strong> controlled by:</p>
<ul>
<li><code>NOMOS_LOG_DIR</code> — Write per-node log files</li>
<li><code>NOMOS_LOG_LEVEL</code> — Global log level (error/warn/info/debug/trace)</li>
<li><code>NOMOS_LOG_FILTER</code> — Target-specific filtering (e.g., <code>consensus=trace,da=debug</code>)</li>
<li><code>NOMOS_TESTS_TRACING</code> — Enable file logging for local runner</li>
</ul>
<p>See <a href="operations.html#logging-and-observability">Logging and Observability</a> for details.</p>
<h2 id="navigation-guide"><a class="header" href="#navigation-guide">Navigation Guide</a></h2>
<div class="table-wrapper"><table><thead><tr><th>To Do This</th><th>Go Here</th></tr></thead><tbody>
<tr><td><strong>Run an example</strong></td><td><code>examples/src/bin/</code> → <code>cargo run -p runner-examples --bin &lt;name&gt;</code></td></tr>
<tr><td><strong>Write a custom scenario</strong></td><td><code>testing-framework/core/</code> → Implement using <code>ScenarioBuilder</code></td></tr>
<tr><td><strong>Add a new workload</strong></td><td><code>testing-framework/workflows/src/workloads/</code> → Implement <code>Workload</code> trait</td></tr>
<tr><td><strong>Add a new expectation</strong></td><td><code>testing-framework/workflows/src/expectations/</code> → Implement <code>Expectation</code> trait</td></tr>
<tr><td><strong>Modify node configs</strong></td><td><code>testing-framework/configs/src/topology/configs/</code></td></tr>
<tr><td><strong>Extend builder DSL</strong></td><td><code>testing-framework/workflows/src/builder/</code> → Add trait methods</td></tr>
<tr><td><strong>Add a new deployer</strong></td><td><code>testing-framework/runners/</code> → Implement <code>Deployer</code> trait</td></tr>
</tbody></table>
</div>
<p>For detailed guidance, see <a href="internal-crate-reference.html">Internal Crate Reference</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="authoring-scenarios"><a class="header" href="#authoring-scenarios">Authoring Scenarios</a></h1>
<p>Creating a scenario is a declarative exercise:</p>
<ol>
<li><strong>Shape the topology</strong>: decide how many validators and executors to run, and
what high-level network and data-availability characteristics matter for the
test.</li>
<li><strong>Attach workloads</strong>: pick traffic generators that align with your goals
(transactions, data-availability blobs, or chaos for resilience probes).</li>
<li><strong>Define expectations</strong>: specify the health signals that must hold when the
run finishes (e.g., consensus liveness, inclusion of submitted activity; see
<a href="workloads.html">Core Content: Workloads &amp; Expectations</a>).</li>
<li><strong>Set duration</strong>: choose a run window long enough to observe meaningful
block progression and the effects of your workloads.</li>
<li><strong>Choose a runner</strong>: target local processes for fast iteration, Docker
Compose for reproducible multi-node stacks, or Kubernetes for cluster-grade
validation. For environment considerations, see <a href="operations.html">Operations</a>.</li>
</ol>
<p>Keep scenarios small and explicit: make the intended behavior and the success
criteria clear so failures are easy to interpret and act upon.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="core-content-workloads--expectations"><a class="header" href="#core-content-workloads--expectations">Core Content: Workloads &amp; Expectations</a></h1>
<p>Workloads describe the activity a scenario generates; expectations describe the
signals that must hold when that activity completes. Both are pluggable so
scenarios stay readable and purpose-driven.</p>
<h2 id="workloads"><a class="header" href="#workloads">Workloads</a></h2>
<ul>
<li><strong>Transaction workload</strong>: submits user-level transactions at a configurable
rate and can limit how many distinct actors participate.</li>
<li><strong>Data-availability workload</strong>: drives blob and channel activity to exercise
data-availability paths.</li>
<li><strong>Chaos workload</strong>: triggers controlled node restarts to test resilience and
recovery behaviors (requires a runner that can control nodes).</li>
</ul>
<h2 id="expectations"><a class="header" href="#expectations">Expectations</a></h2>
<ul>
<li><strong>Consensus liveness</strong>: verifies the system continues to produce blocks in
line with the planned workload and timing window.</li>
<li><strong>Workload-specific checks</strong>: each workload can attach its own success
criteria (e.g., inclusion of submitted activity) so scenarios remain concise.</li>
</ul>
<p>Together, workloads and expectations let you express both the pressure applied
to the system and the definition of “healthy” for that run.</p>
<pre><code class="language-mermaid">flowchart TD
    I[Inputs&lt;br/&gt;topology + wallets + rates] --&gt; Init[Workload init]
    Init --&gt; Drive[Drive traffic]
    Drive --&gt; Collect[Collect signals]
    Collect --&gt; Eval[Expectations evaluate]
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="core-content-scenariobuilderext-patterns"><a class="header" href="#core-content-scenariobuilderext-patterns">Core Content: ScenarioBuilderExt Patterns</a></h1>
<p>Patterns that keep scenarios readable and reusable:</p>
<ul>
<li><strong>Topology-first</strong>: start by shaping the cluster (counts, layout) so later
steps inherit a clear foundation.</li>
<li><strong>Bundle defaults</strong>: use the DSL helpers to attach common expectations (like
liveness) whenever you add a matching workload, reducing forgotten checks.</li>
<li><strong>Intentional rates</strong>: express traffic in per-block terms to align with
protocol timing rather than wall-clock assumptions.</li>
<li><strong>Opt-in chaos</strong>: enable restart patterns only in scenarios meant to probe
resilience; keep functional smoke tests deterministic.</li>
<li><strong>Wallet clarity</strong>: seed only the number of actors you need; it keeps
transaction scenarios deterministic and interpretable.</li>
</ul>
<p>These patterns make scenario definitions self-explanatory while staying aligned
with the framework’s block-oriented timing model.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h1>
<ul>
<li><strong>State your intent</strong>: document the goal of each scenario (throughput, DA
validation, resilience) so expectation choices are obvious.</li>
<li><strong>Keep runs meaningful</strong>: choose durations that allow multiple blocks and make
timing-based assertions trustworthy.</li>
<li><strong>Separate concerns</strong>: start with deterministic workloads for functional
checks; add chaos in dedicated resilience scenarios to avoid noisy failures.</li>
<li><strong>Reuse patterns</strong>: standardize on shared topology and workload presets so
results are comparable across environments and teams.</li>
<li><strong>Observe first, tune second</strong>: rely on liveness and inclusion signals to
interpret outcomes before tweaking rates or topology.</li>
<li><strong>Environment fit</strong>: pick runners that match the feedback loop you need—local
for speed (including fast CI smoke tests), compose for reproducible stacks
(recommended for CI), k8s for cluster-grade fidelity.</li>
<li><strong>Minimal surprises</strong>: seed only necessary wallets and keep configuration
deltas explicit when moving between CI and developer machines.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="examples"><a class="header" href="#examples">Examples</a></h1>
<p>Concrete scenario shapes that illustrate how to combine topologies, workloads,
and expectations.</p>
<p><strong>Runnable examples:</strong> The repo includes complete binaries in <code>examples/src/bin/</code>:</p>
<ul>
<li><code>local_runner.rs</code> — Host processes (local)</li>
<li><code>compose_runner.rs</code> — Docker Compose (requires image built)</li>
<li><code>k8s_runner.rs</code> — Kubernetes (requires cluster access and image loaded)</li>
</ul>
<p><strong>Recommended:</strong> Use <code>scripts/run-examples.sh -t &lt;duration&gt; -v &lt;validators&gt; -e &lt;executors&gt; &lt;mode&gt;</code> where mode is <code>host</code>, <code>compose</code>, or <code>k8s</code>.</p>
<p><strong>Alternative:</strong> Direct cargo run: <code>POL_PROOF_DEV_MODE=true cargo run -p runner-examples --bin &lt;name&gt;</code></p>
<p><strong>All runners require <code>POL_PROOF_DEV_MODE=true</code></strong> to avoid expensive proof generation.</p>
<p><strong>Code patterns</strong> below show how to build scenarios. Wrap these in <code>#[tokio::test]</code> functions for integration tests, or <code>#[tokio::main]</code> for binaries.</p>
<h2 id="simple-consensus-liveness"><a class="header" href="#simple-consensus-liveness">Simple consensus liveness</a></h2>
<p>Minimal test that validates basic block production:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use testing_framework_core::scenario::{Deployer, ScenarioBuilder};
use testing_framework_runner_local::LocalDeployer;
use testing_framework_workflows::ScenarioBuilderExt;
use std::time::Duration;

async fn simple_consensus() -&gt; Result&lt;(), Box&lt;dyn std::error::Error + Send + Sync&gt;&gt; {
    let mut plan = ScenarioBuilder::topology_with(|t| {
            t.network_star()
                .validators(3)
                .executors(0)
        })
        .expect_consensus_liveness()
        .with_run_duration(Duration::from_secs(30))
        .build();

    let deployer = LocalDeployer::default();
    let runner = deployer.deploy(&amp;plan).await?;
    let _handle = runner.run(&amp;mut plan).await?;
    
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>When to use</strong>: smoke tests for consensus on minimal hardware.</p>
<h2 id="transaction-workload"><a class="header" href="#transaction-workload">Transaction workload</a></h2>
<p>Test consensus under transaction load:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use testing_framework_core::scenario::{Deployer, ScenarioBuilder};
use testing_framework_runner_local::LocalDeployer;
use testing_framework_workflows::ScenarioBuilderExt;
use std::time::Duration;

async fn transaction_workload() -&gt; Result&lt;(), Box&lt;dyn std::error::Error + Send + Sync&gt;&gt; {
    let mut plan = ScenarioBuilder::topology_with(|t| {
            t.network_star()
                .validators(2)
                .executors(0)
        })
        .wallets(20)
        .transactions_with(|txs| {
            txs.rate(5)
                .users(10)
        })
        .expect_consensus_liveness()
        .with_run_duration(Duration::from_secs(60))
        .build();

    let deployer = LocalDeployer::default();
    let runner = deployer.deploy(&amp;plan).await?;
    let _handle = runner.run(&amp;mut plan).await?;
    
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>When to use</strong>: validate transaction submission and inclusion.</p>
<h2 id="da--transaction-workload"><a class="header" href="#da--transaction-workload">DA + transaction workload</a></h2>
<p>Combined test stressing both transaction and DA layers:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use testing_framework_core::scenario::{Deployer, ScenarioBuilder};
use testing_framework_runner_local::LocalDeployer;
use testing_framework_workflows::ScenarioBuilderExt;
use std::time::Duration;

async fn da_and_transactions() -&gt; Result&lt;(), Box&lt;dyn std::error::Error + Send + Sync&gt;&gt; {
    let mut plan = ScenarioBuilder::topology_with(|t| {
            t.network_star()
                .validators(3)
                .executors(2)
        })
        .wallets(30)
        .transactions_with(|txs| {
            txs.rate(5)
                .users(15)
        })
        .da_with(|da| {
            da.channel_rate(1)
                .blob_rate(2)
        })
        .expect_consensus_liveness()
        .with_run_duration(Duration::from_secs(90))
        .build();

    let deployer = LocalDeployer::default();
    let runner = deployer.deploy(&amp;plan).await?;
    let _handle = runner.run(&amp;mut plan).await?;
    
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>When to use</strong>: end-to-end coverage of transaction and DA layers.</p>
<h2 id="chaos-resilience"><a class="header" href="#chaos-resilience">Chaos resilience</a></h2>
<p>Test system resilience under node restarts:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use testing_framework_core::scenario::{Deployer, ScenarioBuilder};
use testing_framework_runner_compose::ComposeDeployer;
use testing_framework_workflows::{ScenarioBuilderExt, ChaosBuilderExt};
use std::time::Duration;

async fn chaos_resilience() -&gt; Result&lt;(), Box&lt;dyn std::error::Error + Send + Sync&gt;&gt; {
    let mut plan = ScenarioBuilder::topology_with(|t| {
            t.network_star()
                .validators(4)
                .executors(2)
        })
        .enable_node_control()
        .wallets(20)
        .transactions_with(|txs| {
            txs.rate(3)
                .users(10)
        })
        .chaos_with(|c| {
            c.restart()
                .min_delay(Duration::from_secs(20))
                .max_delay(Duration::from_secs(40))
                .target_cooldown(Duration::from_secs(30))
                .apply()
        })
        .expect_consensus_liveness()
        .with_run_duration(Duration::from_secs(120))
        .build();

    let deployer = ComposeDeployer::default();
    let runner = deployer.deploy(&amp;plan).await?;
    let _handle = runner.run(&amp;mut plan).await?;
    
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>When to use</strong>: resilience validation and operational readiness drills.</p>
<p><strong>Note</strong>: Chaos tests require <code>ComposeDeployer</code> or another runner with node control support.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced-examples"><a class="header" href="#advanced-examples">Advanced Examples</a></h1>
<p>Realistic advanced scenarios demonstrating framework capabilities for production testing.</p>
<h2 id="summary-1"><a class="header" href="#summary-1">Summary</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Example</th><th>Topology</th><th>Workloads</th><th>Deployer</th><th>Key Feature</th></tr></thead><tbody>
<tr><td>Load Progression</td><td>3 validators + 2 executors</td><td>Increasing tx rate</td><td>Compose</td><td>Dynamic load testing</td></tr>
<tr><td>Sustained Load</td><td>4 validators + 2 executors</td><td>High tx + DA rate</td><td>Compose</td><td>Stress testing</td></tr>
<tr><td>Aggressive Chaos</td><td>4 validators + 2 executors</td><td>Frequent restarts + traffic</td><td>Compose</td><td>Resilience validation</td></tr>
</tbody></table>
</div>
<h2 id="load-progression-test"><a class="header" href="#load-progression-test">Load Progression Test</a></h2>
<p>Test consensus under progressively increasing transaction load:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use testing_framework_core::scenario::{Deployer, ScenarioBuilder};
use testing_framework_runner_compose::ComposeDeployer;
use testing_framework_workflows::ScenarioBuilderExt;
use std::time::Duration;

async fn load_progression_test() -&gt; Result&lt;(), Box&lt;dyn std::error::Error + Send + Sync&gt;&gt; {
    for rate in [5, 10, 20, 30] {
        println!("Testing with rate: {}", rate);
        
        let mut plan = ScenarioBuilder::topology_with(|t| {
                t.network_star()
                    .validators(3)
                    .executors(2)
            })
            .wallets(50)
            .transactions_with(|txs| {
                txs.rate(rate)
                    .users(20)
            })
            .expect_consensus_liveness()
            .with_run_duration(Duration::from_secs(60))
            .build();

        let deployer = ComposeDeployer::default();
        let runner = deployer.deploy(&amp;plan).await?;
        let _handle = runner.run(&amp;mut plan).await?;
    }
    
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>When to use:</strong> Finding the maximum sustainable transaction rate for a given topology.</p>
<h2 id="sustained-load-test"><a class="header" href="#sustained-load-test">Sustained Load Test</a></h2>
<p>Run high transaction and DA load for extended duration:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use testing_framework_core::scenario::{Deployer, ScenarioBuilder};
use testing_framework_runner_compose::ComposeDeployer;
use testing_framework_workflows::ScenarioBuilderExt;
use std::time::Duration;

async fn sustained_load_test() -&gt; Result&lt;(), Box&lt;dyn std::error::Error + Send + Sync&gt;&gt; {
    let mut plan = ScenarioBuilder::topology_with(|t| {
            t.network_star()
                .validators(4)
                .executors(2)
        })
        .wallets(100)
        .transactions_with(|txs| {
            txs.rate(15)
                .users(50)
        })
        .da_with(|da| {
            da.channel_rate(2)
                .blob_rate(3)
        })
        .expect_consensus_liveness()
        .with_run_duration(Duration::from_secs(300))
        .build();

    let deployer = ComposeDeployer::default();
    let runner = deployer.deploy(&amp;plan).await?;
    let _handle = runner.run(&amp;mut plan).await?;
    
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>When to use:</strong> Validating stability under continuous high load over extended periods.</p>
<h2 id="aggressive-chaos-test"><a class="header" href="#aggressive-chaos-test">Aggressive Chaos Test</a></h2>
<p>Frequent node restarts with active traffic:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use testing_framework_core::scenario::{Deployer, ScenarioBuilder};
use testing_framework_runner_compose::ComposeDeployer;
use testing_framework_workflows::{ScenarioBuilderExt, ChaosBuilderExt};
use std::time::Duration;

async fn aggressive_chaos_test() -&gt; Result&lt;(), Box&lt;dyn std::error::Error + Send + Sync&gt;&gt; {
    let mut plan = ScenarioBuilder::topology_with(|t| {
            t.network_star()
                .validators(4)
                .executors(2)
        })
        .enable_node_control()
        .wallets(50)
        .transactions_with(|txs| {
            txs.rate(10)
                .users(20)
        })
        .chaos_with(|c| {
            c.restart()
                .min_delay(Duration::from_secs(10))
                .max_delay(Duration::from_secs(20))
                .target_cooldown(Duration::from_secs(15))
                .apply()
        })
        .expect_consensus_liveness()
        .with_run_duration(Duration::from_secs(180))
        .build();

    let deployer = ComposeDeployer::default();
    let runner = deployer.deploy(&amp;plan).await?;
    let _handle = runner.run(&amp;mut plan).await?;
    
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>When to use:</strong> Validating recovery and liveness under aggressive failure conditions.</p>
<p><strong>Note:</strong> Requires <code>ComposeDeployer</code> for node control support.</p>
<h2 id="extension-ideas"><a class="header" href="#extension-ideas">Extension Ideas</a></h2>
<p>These scenarios require custom implementations but demonstrate framework extensibility:</p>
<h3 id="mempool--transaction-handling"><a class="header" href="#mempool--transaction-handling">Mempool &amp; Transaction Handling</a></h3>
<h4 id="transaction-propagation--inclusion-test"><a class="header" href="#transaction-propagation--inclusion-test">Transaction Propagation &amp; Inclusion Test</a></h4>
<p><strong>Concept:</strong> Submit the same batch of independent transactions to different nodes in randomized order/offsets, then verify all transactions are included and final state matches across nodes.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li><strong>Custom workload:</strong> Generates a fixed batch of transactions and submits the same set to different nodes via <code>ctx.node_clients()</code>, with randomized submission order and timing offsets per node</li>
<li><strong>Custom expectation:</strong> Verifies all transactions appear in blocks (order may vary), final state matches across all nodes (compare balances or state roots), and no transactions are dropped</li>
</ul>
<p><strong>Why useful:</strong> Exercises mempool propagation, proposer fairness, and transaction inclusion guarantees under realistic race conditions. Tests that the protocol maintains consistency regardless of which node receives transactions first.</p>
<p><strong>Implementation notes:</strong> Requires both a custom <code>Workload</code> implementation (to submit same transactions to multiple nodes with jitter) and a custom <code>Expectation</code> implementation (to verify inclusion and state consistency).</p>
<h4 id="cross-validator-mempool-divergence--convergence"><a class="header" href="#cross-validator-mempool-divergence--convergence">Cross-Validator Mempool Divergence &amp; Convergence</a></h4>
<p><strong>Concept:</strong> Drive different transaction subsets into different validators (or differing arrival orders) to create temporary mempool divergence, then verify mempools/blocks converge to contain the union (no permanent divergence).</p>
<p><strong>Requirements:</strong></p>
<ul>
<li><strong>Custom workload:</strong> Targets specific nodes via <code>ctx.node_clients()</code> with disjoint or jittered transaction batches</li>
<li><strong>Custom expectation:</strong> After a convergence window, verifies that all transactions appear in blocks (order may vary) or that mempool contents converge across nodes</li>
<li>Run normal workloads during convergence period</li>
</ul>
<p><strong>Expectations:</strong></p>
<ul>
<li>Temporary mempool divergence is acceptable (different nodes see different transactions initially)</li>
<li>After convergence window, all transactions appear in blocks or mempools converge</li>
<li>No transactions are permanently dropped despite initial divergence</li>
<li>Mempool gossip/reconciliation mechanisms work correctly</li>
</ul>
<p><strong>Why useful:</strong> Exercises mempool gossip and reconciliation under uneven input or latency. Ensures no node "drops" transactions seen elsewhere, validating that mempool synchronization mechanisms correctly propagate transactions across the network even when they arrive at different nodes in different orders.</p>
<p><strong>Implementation notes:</strong> Requires both a custom <code>Workload</code> implementation (to inject disjoint/jittered batches per node) and a custom <code>Expectation</code> implementation (to verify mempool convergence or block inclusion). Uses existing <code>ctx.node_clients()</code> capability—no new infrastructure needed.</p>
<h4 id="adaptive-mempool-pressure-test"><a class="header" href="#adaptive-mempool-pressure-test">Adaptive Mempool Pressure Test</a></h4>
<p><strong>Concept:</strong> Ramp transaction load over time to observe mempool growth, fee prioritization/eviction, and block saturation behavior, detecting performance regressions and ensuring backpressure/eviction work under increasing load.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li><strong>Custom workload:</strong> Steadily increases transaction rate over time (optional: use fee tiers)</li>
<li><strong>Custom expectation:</strong> Monitors mempool size, evictions, and throughput (blocks/txs per slot), flagging runaway growth or stalls</li>
<li>Run for extended duration to observe pressure buildup</li>
</ul>
<p><strong>Expectations:</strong></p>
<ul>
<li>Mempool size grows predictably with load (not runaway growth)</li>
<li>Fee prioritization/eviction mechanisms activate under pressure</li>
<li>Block saturation behavior is acceptable (blocks fill appropriately)</li>
<li>Throughput (blocks/txs per slot) remains stable or degrades gracefully</li>
<li>No stalls or unbounded mempool growth</li>
</ul>
<p><strong>Why useful:</strong> Detects performance regressions in mempool management. Ensures backpressure and eviction mechanisms work correctly under increasing load, preventing memory exhaustion or unbounded growth. Validates that fee prioritization correctly selects high-value transactions when mempool is full.</p>
<p><strong>Implementation notes:</strong> Can be built with current workload model (ramping rate). Requires custom <code>Expectation</code> implementation that reads mempool metrics (via node HTTP APIs or Prometheus) and monitors throughput to judge behavior. No new infrastructure needed—uses existing observability capabilities.</p>
<h4 id="invalid-transaction-fuzzing"><a class="header" href="#invalid-transaction-fuzzing">Invalid Transaction Fuzzing</a></h4>
<p><strong>Concept:</strong> Submit malformed transactions and verify they're rejected properly.</p>
<p><strong>Implementation approach:</strong></p>
<ul>
<li>Custom workload that generates invalid transactions (bad signatures, insufficient funds, malformed structure)</li>
<li>Expectation verifies mempool rejects them and they never appear in blocks</li>
<li>Test mempool resilience and filtering</li>
</ul>
<p><strong>Why useful:</strong> Ensures mempool doesn't crash or include invalid transactions under fuzzing.</p>
<h3 id="network--gossip"><a class="header" href="#network--gossip">Network &amp; Gossip</a></h3>
<h4 id="gossip-latency-gradient-scenario"><a class="header" href="#gossip-latency-gradient-scenario">Gossip Latency Gradient Scenario</a></h4>
<p><strong>Concept:</strong> Test consensus robustness under skewed gossip delays by partitioning nodes into latency tiers (tier A ≈10ms, tier B ≈100ms, tier C ≈300ms) and observing propagation lag, fork rate, and eventual convergence.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li>Partition nodes into three groups (tiers)</li>
<li>Apply per-group network delay via chaos: <code>netem</code>/<code>iptables</code> in compose; NetworkPolicy + <code>netem</code> sidecar in k8s</li>
<li>Run standard workload (transactions/block production)</li>
<li>Optional: Remove delays at end to check recovery</li>
</ul>
<p><strong>Expectations:</strong></p>
<ul>
<li><strong>Propagation:</strong> Messages reach all tiers within acceptable bounds</li>
<li><strong>Safety:</strong> No divergent finalized heads; fork rate stays within tolerance</li>
<li><strong>Liveness:</strong> Chain keeps advancing; convergence after delays relaxed (if healed)</li>
</ul>
<p><strong>Why useful:</strong> Real networks have heterogeneous latency. This stress-tests proposer selection and fork resolution when some peers are "far" (high latency), validating that consensus remains safe and live under realistic network conditions.</p>
<p><strong>Current blocker:</strong> Runner support for per-group delay injection (network delay via <code>netem</code>/<code>iptables</code>) is not present today. Would require new chaos plumbing in compose/k8s deployers to inject network delays per node group.</p>
<h4 id="byzantine-gossip-flooding-libp2p-peer"><a class="header" href="#byzantine-gossip-flooding-libp2p-peer">Byzantine Gossip Flooding (libp2p Peer)</a></h4>
<p><strong>Concept:</strong> Spin up a custom workload/sidecar that runs a libp2p host, joins the cluster's gossip mesh, and publishes a high rate of syntactically valid but useless/stale messages to selected topics, testing gossip backpressure, scoring, and queue handling under a "malicious" peer.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li>Custom workload/sidecar that implements a libp2p host</li>
<li>Join the cluster's gossip mesh as a peer</li>
<li>Publish high-rate syntactically valid but useless/stale messages to selected gossip topics</li>
<li>Run alongside normal workloads (transactions/block production)</li>
</ul>
<p><strong>Expectations:</strong></p>
<ul>
<li>Gossip backpressure mechanisms prevent message flooding from overwhelming nodes</li>
<li>Peer scoring correctly identifies and penalizes the malicious peer</li>
<li>Queue handling remains stable under flood conditions</li>
<li>Normal consensus operation continues despite malicious peer</li>
</ul>
<p><strong>Why useful:</strong> Tests Byzantine behavior (malicious peer) which is critical for consensus protocol robustness. More realistic than RPC spam since it uses the actual gossip protocol. Validates that gossip backpressure, peer scoring, and queue management correctly handle adversarial peers without disrupting consensus.</p>
<p><strong>Current blocker:</strong> Requires adding gossip-capable helper (libp2p integration) to the framework. Would need a custom workload/sidecar implementation that can join the gossip mesh and inject messages. The rest of the scenario can use existing runners/workloads.</p>
<h4 id="network-partition-recovery"><a class="header" href="#network-partition-recovery">Network Partition Recovery</a></h4>
<p><strong>Concept:</strong> Test consensus recovery after network partitions.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li>Needs <code>block_peer()</code> / <code>unblock_peer()</code> methods in <code>NodeControlHandle</code></li>
<li>Partition subsets of validators, wait, then restore connectivity</li>
<li>Verify chain convergence after partition heals</li>
</ul>
<p><strong>Why useful:</strong> Tests the most realistic failure mode in distributed systems.</p>
<p><strong>Current blocker:</strong> Node control doesn't yet support network-level actions (only process restarts).</p>
<h3 id="time--timing"><a class="header" href="#time--timing">Time &amp; Timing</a></h3>
<h4 id="time-shifted-blocks-clock-skew-test"><a class="header" href="#time-shifted-blocks-clock-skew-test">Time-Shifted Blocks (Clock Skew Test)</a></h4>
<p><strong>Concept:</strong> Test consensus and timestamp handling when nodes run with skewed clocks (e.g., +1s, −1s, +200ms jitter) to surface timestamp validation issues, reorg sensitivity, and clock drift handling.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li>Assign per-node time offsets (e.g., +1s, −1s, +200ms jitter)</li>
<li>Run normal workload (transactions/block production)</li>
<li>Observe whether blocks are accepted/propagated and the chain stays consistent</li>
</ul>
<p><strong>Expectations:</strong></p>
<ul>
<li>Blocks with skewed timestamps are handled correctly (accepted or rejected per protocol rules)</li>
<li>Chain remains consistent across nodes despite clock differences</li>
<li>No unexpected reorgs or chain splits due to timestamp validation issues</li>
</ul>
<p><strong>Why useful:</strong> Clock skew is a common real-world issue in distributed systems. This validates that consensus correctly handles timestamp validation and maintains safety/liveness when nodes have different clock offsets, preventing timestamp-based attacks or failures.</p>
<p><strong>Current blocker:</strong> Runner ability to skew per-node clocks (e.g., privileged containers with <code>libfaketime</code>/<code>chrony</code> or time-offset netns) is not available today. Would require a new chaos/time-skew hook in deployers to inject clock offsets per node.</p>
<h4 id="block-timing-consistency"><a class="header" href="#block-timing-consistency">Block Timing Consistency</a></h4>
<p><strong>Concept:</strong> Verify block production intervals stay within expected bounds.</p>
<p><strong>Implementation approach:</strong></p>
<ul>
<li>Custom expectation that consumes <code>BlockFeed</code></li>
<li>Collect block timestamps during run</li>
<li>Assert intervals are within <code>(slot_duration * active_slot_coeff) ± tolerance</code></li>
</ul>
<p><strong>Why useful:</strong> Validates consensus timing under various loads.</p>
<h3 id="topology--membership"><a class="header" href="#topology--membership">Topology &amp; Membership</a></h3>
<h4 id="dynamic-topology-churn-scenario"><a class="header" href="#dynamic-topology-churn-scenario">Dynamic Topology (Churn) Scenario</a></h4>
<p><strong>Concept:</strong> Nodes join and leave mid-run (new identities/addresses added; some nodes permanently removed) to exercise peer discovery, bootstrapping, reputation, and load balancing under churn.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li>Runner must be able to spin up new nodes with fresh keys/addresses at runtime</li>
<li>Update peer lists and bootstraps dynamically as nodes join/leave</li>
<li>Optionally tear down nodes permanently (not just restart)</li>
<li>Run normal workloads (transactions/block production) during churn</li>
</ul>
<p><strong>Expectations:</strong></p>
<ul>
<li>New nodes successfully discover and join the network</li>
<li>Peer discovery mechanisms correctly handle dynamic topology changes</li>
<li>Reputation systems adapt to new/removed peers</li>
<li>Load balancing adjusts to changing node set</li>
<li>Consensus remains safe and live despite topology churn</li>
</ul>
<p><strong>Why useful:</strong> Real networks experience churn (nodes joining/leaving). Unlike restarts (which preserve topology), churn changes the actual topology size and peer set, testing how the protocol handles dynamic membership. This exercises peer discovery, bootstrapping, reputation systems, and load balancing under realistic conditions.</p>
<p><strong>Current blocker:</strong> Runner support for dynamic node addition/removal at runtime is not available today. Chaos today only restarts existing nodes; churn would require the ability to spin up new nodes with fresh identities/addresses, update peer lists/bootstraps dynamically, and permanently remove nodes. Would need new topology management capabilities in deployers.</p>
<h3 id="api--external-interfaces"><a class="header" href="#api--external-interfaces">API &amp; External Interfaces</a></h3>
<h4 id="api-dosstress-test"><a class="header" href="#api-dosstress-test">API DoS/Stress Test</a></h4>
<p><strong>Concept:</strong> Adversarial workload floods node HTTP/WS APIs with high QPS and malformed/bursty requests; expectation checks nodes remain responsive or rate-limit without harming consensus.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li><strong>Custom workload:</strong> Targets node HTTP/WS API endpoints with mixed valid/invalid requests at high rate</li>
<li><strong>Custom expectation:</strong> Monitors error rates, latency, and confirms block production/liveness unaffected</li>
<li>Run alongside normal workloads (transactions/block production)</li>
</ul>
<p><strong>Expectations:</strong></p>
<ul>
<li>Nodes remain responsive or correctly rate-limit under API flood</li>
<li>Error rates/latency are acceptable (rate limiting works)</li>
<li>Block production/liveness unaffected by API abuse</li>
<li>Consensus continues normally despite API stress</li>
</ul>
<p><strong>Why useful:</strong> Validates API hardening under abuse and ensures control/telemetry endpoints don't destabilize the node. Tests that API abuse is properly isolated from consensus operations, preventing DoS attacks on API endpoints from affecting blockchain functionality.</p>
<p><strong>Implementation notes:</strong> Requires custom <code>Workload</code> implementation that directs high-QPS traffic to node APIs (via <code>ctx.node_clients()</code> or direct HTTP clients) and custom <code>Expectation</code> implementation that monitors API responsiveness metrics and consensus liveness. Uses existing node API access—no new infrastructure needed.</p>
<h3 id="state--correctness"><a class="header" href="#state--correctness">State &amp; Correctness</a></h3>
<h4 id="wallet-balance-verification"><a class="header" href="#wallet-balance-verification">Wallet Balance Verification</a></h4>
<p><strong>Concept:</strong> Track wallet balances and verify state consistency.</p>
<p><strong>Description:</strong> After transaction workload completes, query all wallet balances via node API and verify total supply is conserved. Requires tracking initial state, submitted transactions, and final balances. Validates that the ledger maintains correctness under load (no funds lost or created). This is a <strong>state assertion</strong> expectation that checks correctness, not just liveness.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="running-scenarios"><a class="header" href="#running-scenarios">Running Scenarios</a></h1>
<p>Running a scenario follows the same conceptual flow regardless of environment:</p>
<ol>
<li>Select or author a scenario plan that pairs a topology with workloads,
expectations, and a suitable run window.</li>
<li>Choose a deployer aligned with your environment (local, compose, or k8s) and
ensure its prerequisites are available.</li>
<li>Deploy the plan through the deployer, which provisions infrastructure and
returns a runner.</li>
<li>The runner orchestrates workload execution for the planned duration; keep
observability signals visible so you can correlate outcomes.</li>
<li>The runner evaluates expectations and captures results as the primary
pass/fail signal.</li>
</ol>
<p>Use the same plan across different deployers to compare behavior between local
development and CI or cluster settings. For environment prerequisites and
flags, see <a href="operations.html">Operations</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="runners"><a class="header" href="#runners">Runners</a></h1>
<p>Runners turn a scenario plan into a live environment while keeping the plan
unchanged. Choose based on feedback speed, reproducibility, and fidelity. For
environment and operational considerations, see <a href="operations.html">Operations</a>.</p>
<p><strong>Important:</strong> All runners require <code>POL_PROOF_DEV_MODE=true</code> to avoid expensive Groth16 proof generation that causes timeouts.</p>
<h2 id="host-runner-local-processes"><a class="header" href="#host-runner-local-processes">Host runner (local processes)</a></h2>
<ul>
<li>Launches node processes directly on the host (via <code>LocalDeployer</code>).</li>
<li>Binary: <code>local_runner.rs</code>, script mode: <code>host</code></li>
<li>Fastest feedback loop and minimal orchestration overhead.</li>
<li>Best for development-time iteration and debugging.</li>
<li><strong>Can run in CI</strong> for fast smoke tests.</li>
<li><strong>Node control:</strong> Not supported (chaos workloads not available)</li>
</ul>
<p><strong>Run with:</strong> <code>scripts/run-examples.sh -t 60 -v 1 -e 1 host</code></p>
<h2 id="docker-compose-runner"><a class="header" href="#docker-compose-runner">Docker Compose runner</a></h2>
<ul>
<li>Starts nodes in containers to provide a reproducible multi-node stack on a
single machine (via <code>ComposeDeployer</code>).</li>
<li>Binary: <code>compose_runner.rs</code>, script mode: <code>compose</code></li>
<li>Discovers service ports and wires observability for convenient inspection.</li>
<li>Good balance between fidelity and ease of setup.</li>
<li><strong>Recommended for CI pipelines</strong> (isolated environment, reproducible).</li>
<li><strong>Node control:</strong> Supported (can restart nodes for chaos testing)</li>
</ul>
<p><strong>Run with:</strong> <code>scripts/run-examples.sh -t 60 -v 1 -e 1 compose</code></p>
<h2 id="kubernetes-runner"><a class="header" href="#kubernetes-runner">Kubernetes runner</a></h2>
<ul>
<li>Deploys nodes onto a cluster for higher-fidelity, longer-running scenarios (via <code>K8sDeployer</code>).</li>
<li>Binary: <code>k8s_runner.rs</code>, script mode: <code>k8s</code></li>
<li>Suits CI with cluster access or shared test environments where cluster behavior
and scheduling matter.</li>
<li><strong>Node control:</strong> Not supported yet (chaos workloads not available)</li>
</ul>
<p><strong>Run with:</strong> <code>scripts/run-examples.sh -t 60 -v 1 -e 1 k8s</code></p>
<h3 id="common-expectations"><a class="header" href="#common-expectations">Common expectations</a></h3>
<ul>
<li>All runners require at least one validator and, for transaction scenarios,
access to seeded wallets.</li>
<li>Readiness probes gate workload start so traffic begins only after nodes are
reachable.</li>
<li>Environment flags can relax timeouts or increase tracing when diagnostics are
needed.</li>
</ul>
<pre><code class="language-mermaid">flowchart TD
    Plan[Scenario Plan] --&gt; RunSel[Runner&lt;br/&gt;host, compose, or k8s]
    RunSel --&gt; Provision[Provision &amp; readiness]
    Provision --&gt; Runtime[Runtime + observability]
    Runtime --&gt; Exec[Workloads &amp; Expectations execute]
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="operations"><a class="header" href="#operations">Operations</a></h1>
<p>Operational readiness focuses on prerequisites, environment fit, and clear
signals:</p>
<ul>
<li><strong>Prerequisites</strong>:
<ul>
<li><strong><code>versions.env</code> file</strong> at repository root (required by helper scripts; defines VERSION, NOMOS_NODE_REV, NOMOS_BUNDLE_VERSION)</li>
<li>Keep a sibling <code>nomos-node</code> checkout available, or use <code>scripts/run-examples.sh</code> which clones/builds on demand</li>
<li>Ensure the chosen runner's platform needs are met (Docker for compose, cluster access for k8s)</li>
<li>CI uses prebuilt binary artifacts from the <code>build-binaries</code> workflow</li>
</ul>
</li>
<li><strong>Artifacts</strong>: DA scenarios require KZG parameters (circuit assets) located at
<code>testing-framework/assets/stack/kzgrs_test_params</code>. Fetch them via
<code>scripts/setup-nomos-circuits.sh</code> or override the path with <code>NOMOS_KZGRS_PARAMS_PATH</code>.</li>
<li><strong>Environment flags</strong>: <code>POL_PROOF_DEV_MODE=true</code> is <strong>required for all runners</strong>
(local, compose, k8s) unless you want expensive Groth16 proof generation that
will cause tests to timeout. Configure logging via <code>NOMOS_LOG_DIR</code>, <code>NOMOS_LOG_LEVEL</code>,
and <code>NOMOS_LOG_FILTER</code> (see <a href="operations.html#logging-and-observability">Logging and Observability</a>
for details). Note that nodes ignore <code>RUST_LOG</code> and only respond to <code>NOMOS_*</code> variables.</li>
<li><strong>Readiness checks</strong>: verify runners report node readiness before starting
workloads; this avoids false negatives from starting too early.</li>
<li><strong>Failure triage</strong>: map failures to missing prerequisites (wallet seeding,
node control availability), runner platform issues, or unmet expectations.
Start with liveness signals, then dive into workload-specific assertions.</li>
</ul>
<p>Treat operational hygiene—assets present, prerequisites satisfied, observability
reachable—as the first step to reliable scenario outcomes.</p>
<h2 id="ci-usage"><a class="header" href="#ci-usage">CI Usage</a></h2>
<p>Both <strong>LocalDeployer</strong> and <strong>ComposeDeployer</strong> work in CI environments:</p>
<p><strong>LocalDeployer in CI:</strong></p>
<ul>
<li>Faster (no Docker overhead)</li>
<li>Good for quick smoke tests</li>
<li><strong>Trade-off:</strong> Less isolation (processes share host)</li>
</ul>
<p><strong>ComposeDeployer in CI (recommended):</strong></p>
<ul>
<li>Better isolation (containerized)</li>
<li>Reproducible environment</li>
<li>Includes Prometheus/observability</li>
<li><strong>Trade-off:</strong> Slower startup (Docker image build)</li>
<li><strong>Trade-off:</strong> Requires Docker daemon</li>
</ul>
<p>See <code>.github/workflows/compose-mixed.yml</code> for a complete CI example using ComposeDeployer.</p>
<h2 id="running-examples"><a class="header" href="#running-examples">Running Examples</a></h2>
<p>The framework provides three runner modes: <strong>host</strong> (local processes), <strong>compose</strong> (Docker Compose), and <strong>k8s</strong> (Kubernetes).</p>
<p><strong>Recommended:</strong> Use <code>scripts/run-examples.sh</code> for all modes:</p>
<pre><code class="language-bash"># Host mode (local processes)
scripts/run-examples.sh -t 60 -v 1 -e 1 host

# Compose mode (Docker Compose)
scripts/run-examples.sh -t 60 -v 1 -e 1 compose

# K8s mode (Kubernetes)
scripts/run-examples.sh -t 60 -v 1 -e 1 k8s
</code></pre>
<p>This script handles circuit setup, binary building/bundling, image building, and execution.</p>
<p><strong>Environment overrides:</strong></p>
<ul>
<li><code>VERSION=v0.3.1</code> — Circuit version</li>
<li><code>NOMOS_NODE_REV=&lt;commit&gt;</code> — nomos-node git revision</li>
<li><code>NOMOS_BINARIES_TAR=path/to/bundle.tar.gz</code> — Use prebuilt bundle</li>
<li><code>NOMOS_SKIP_IMAGE_BUILD=1</code> — Skip image rebuild (compose/k8s)</li>
</ul>
<h3 id="host-runner-direct-cargo-run"><a class="header" href="#host-runner-direct-cargo-run">Host Runner (Direct Cargo Run)</a></h3>
<p>For manual control, you can run the <code>local_runner</code> binary directly:</p>
<pre><code class="language-bash">POL_PROOF_DEV_MODE=true \
NOMOS_NODE_BIN=/path/to/nomos-node \
NOMOS_EXECUTOR_BIN=/path/to/nomos-executor \
cargo run -p runner-examples --bin local_runner
</code></pre>
<p><strong>Environment variables:</strong></p>
<ul>
<li><code>NOMOS_DEMO_VALIDATORS=3</code> — Number of validators (default: 1, or use legacy <code>LOCAL_DEMO_VALIDATORS</code>)</li>
<li><code>NOMOS_DEMO_EXECUTORS=2</code> — Number of executors (default: 1, or use legacy <code>LOCAL_DEMO_EXECUTORS</code>)</li>
<li><code>NOMOS_DEMO_RUN_SECS=120</code> — Run duration in seconds (default: 60, or use legacy <code>LOCAL_DEMO_RUN_SECS</code>)</li>
<li><code>NOMOS_NODE_BIN</code> / <code>NOMOS_EXECUTOR_BIN</code> — Paths to binaries (required for direct run)</li>
<li><code>NOMOS_TESTS_TRACING=true</code> — Enable persistent file logging</li>
<li><code>NOMOS_LOG_DIR=/tmp/logs</code> — Directory for per-node log files</li>
<li><code>NOMOS_LOG_LEVEL=debug</code> — Set log level (default: info)</li>
<li><code>NOMOS_LOG_FILTER=consensus=trace,da=debug</code> — Fine-grained module filtering</li>
</ul>
<p><strong>Note:</strong> Requires circuit assets and host binaries. Use <code>scripts/run-examples.sh host</code> to handle setup automatically.</p>
<h3 id="compose-runner-direct-cargo-run"><a class="header" href="#compose-runner-direct-cargo-run">Compose Runner (Direct Cargo Run)</a></h3>
<p>For manual control, you can run the <code>compose_runner</code> binary directly. Compose requires a Docker image with embedded assets.</p>
<p><strong>Recommended setup:</strong> Use a prebuilt bundle:</p>
<pre><code class="language-bash"># Build a Linux bundle (includes binaries + circuits)
scripts/build-bundle.sh --platform linux
# Creates .tmp/nomos-binaries-linux-v0.3.1.tar.gz

# Build image (embeds bundle assets)
export NOMOS_BINARIES_TAR=.tmp/nomos-binaries-linux-v0.3.1.tar.gz
testing-framework/assets/stack/scripts/build_test_image.sh

# Run
NOMOS_TESTNET_IMAGE=nomos-testnet:local \
POL_PROOF_DEV_MODE=true \
cargo run -p runner-examples --bin compose_runner
</code></pre>
<p><strong>Alternative:</strong> Manual circuit/image setup (rebuilds during image build):</p>
<pre><code class="language-bash"># Fetch and copy circuits
scripts/setup-nomos-circuits.sh v0.3.1 /tmp/nomos-circuits
cp -r /tmp/nomos-circuits/* testing-framework/assets/stack/kzgrs_test_params/

# Build image
testing-framework/assets/stack/scripts/build_test_image.sh

# Run
NOMOS_TESTNET_IMAGE=nomos-testnet:local \
POL_PROOF_DEV_MODE=true \
cargo run -p runner-examples --bin compose_runner
</code></pre>
<p><strong>Environment variables:</strong></p>
<ul>
<li><code>NOMOS_TESTNET_IMAGE=nomos-testnet:local</code> — Image tag (required, must match built image)</li>
<li><code>POL_PROOF_DEV_MODE=true</code> — <strong>Required</strong> for all runners</li>
<li><code>NOMOS_DEMO_VALIDATORS=3</code> / <code>NOMOS_DEMO_EXECUTORS=2</code> / <code>NOMOS_DEMO_RUN_SECS=120</code> — Topology overrides</li>
<li><code>COMPOSE_NODE_PAIRS=1x1</code> — Alternative topology format: "validators×executors"</li>
<li><code>TEST_FRAMEWORK_PROMETHEUS_PORT=9091</code> — Override Prometheus port (default: 9090)</li>
<li><code>COMPOSE_RUNNER_HOST=127.0.0.1</code> — Host address for port mappings</li>
<li><code>COMPOSE_RUNNER_PRESERVE=1</code> — Keep containers running after test</li>
<li><code>NOMOS_LOG_DIR=/tmp/compose-logs</code> — Write logs to files inside containers</li>
</ul>
<p><strong>Compose-specific features:</strong></p>
<ul>
<li><strong>Node control support</strong>: Only runner that supports chaos testing (<code>.enable_node_control()</code> + chaos workloads)</li>
<li><strong>Prometheus observability</strong>: Metrics at <code>http://localhost:9090</code></li>
</ul>
<p><strong>Important:</strong></p>
<ul>
<li>Containers expect KZG parameters at <code>/kzgrs_test_params/kzgrs_test_params</code> (note the repeated filename)</li>
<li>Use <code>scripts/run-examples.sh compose</code> to handle all setup automatically</li>
</ul>
<h3 id="k8s-runner-direct-cargo-run"><a class="header" href="#k8s-runner-direct-cargo-run">K8s Runner (Direct Cargo Run)</a></h3>
<p>For manual control, you can run the <code>k8s_runner</code> binary directly. K8s requires the same image setup as Compose.</p>
<p><strong>Prerequisites:</strong></p>
<ol>
<li><strong>Kubernetes cluster</strong> with <code>kubectl</code> configured</li>
<li><strong>Test image built</strong> (same as Compose, preferably with prebuilt bundle)</li>
<li><strong>Image available in cluster</strong> (loaded or pushed to registry)</li>
</ol>
<p><strong>Build and load image:</strong></p>
<pre><code class="language-bash"># Build image with bundle (recommended)
scripts/build-bundle.sh --platform linux
export NOMOS_BINARIES_TAR=.tmp/nomos-binaries-linux-v0.3.1.tar.gz
testing-framework/assets/stack/scripts/build_test_image.sh

# Load into cluster
export NOMOS_TESTNET_IMAGE=nomos-testnet:local
kind load docker-image nomos-testnet:local  # For kind
# OR: minikube image load nomos-testnet:local  # For minikube
# OR: docker push your-registry/nomos-testnet:local  # For remote
</code></pre>
<p><strong>Run the example:</strong></p>
<pre><code class="language-bash">export NOMOS_TESTNET_IMAGE=nomos-testnet:local
export POL_PROOF_DEV_MODE=true
cargo run -p runner-examples --bin k8s_runner
</code></pre>
<p><strong>Environment variables:</strong></p>
<ul>
<li><code>NOMOS_TESTNET_IMAGE</code> — Image tag (required)</li>
<li><code>POL_PROOF_DEV_MODE=true</code> — <strong>Required</strong> for all runners</li>
<li><code>NOMOS_DEMO_VALIDATORS</code> / <code>NOMOS_DEMO_EXECUTORS</code> / <code>NOMOS_DEMO_RUN_SECS</code> — Topology overrides</li>
</ul>
<p><strong>Important:</strong></p>
<ul>
<li>K8s runner mounts <code>testing-framework/assets/stack/kzgrs_test_params</code> as a hostPath volume with file <code>/kzgrs_test_params/kzgrs_test_params</code> inside pods</li>
<li><strong>No node control support yet</strong>: Chaos workloads (<code>.enable_node_control()</code>) will fail</li>
<li>Use <code>scripts/run-examples.sh k8s</code> to handle all setup automatically</li>
</ul>
<h2 id="circuit-assets-kzg-parameters"><a class="header" href="#circuit-assets-kzg-parameters">Circuit Assets (KZG Parameters)</a></h2>
<p>DA workloads require KZG cryptographic parameters for polynomial commitment schemes.</p>
<h3 id="asset-location"><a class="header" href="#asset-location">Asset Location</a></h3>
<p><strong>Default path:</strong> <code>testing-framework/assets/stack/kzgrs_test_params/kzgrs_test_params</code></p>
<p>Note the repeated filename: the directory <code>kzgrs_test_params/</code> contains a file named <code>kzgrs_test_params</code>. This is the actual proving key file.</p>
<p><strong>Container path</strong> (compose/k8s): <code>/kzgrs_test_params/kzgrs_test_params</code></p>
<p><strong>Override:</strong> Set <code>NOMOS_KZGRS_PARAMS_PATH</code> to use a custom location (must point to the file):</p>
<pre><code class="language-bash">NOMOS_KZGRS_PARAMS_PATH=/path/to/custom/params cargo run -p runner-examples --bin local_runner
</code></pre>
<h3 id="getting-circuit-assets"><a class="header" href="#getting-circuit-assets">Getting Circuit Assets</a></h3>
<p><strong>Option 1: Use helper script</strong> (recommended):</p>
<pre><code class="language-bash"># From the repository root
chmod +x scripts/setup-nomos-circuits.sh
scripts/setup-nomos-circuits.sh v0.3.1 /tmp/nomos-circuits

# Copy to default location
cp -r /tmp/nomos-circuits/* testing-framework/assets/stack/kzgrs_test_params/
</code></pre>
<p><strong>Option 2: Build locally</strong> (advanced):</p>
<pre><code class="language-bash"># Requires Go, Rust, and circuit build tools
make kzgrs_test_params
</code></pre>
<h3 id="ci-workflow"><a class="header" href="#ci-workflow">CI Workflow</a></h3>
<p>The CI automatically fetches and places assets:</p>
<pre><code class="language-yaml">- name: Install circuits for host build
  run: |
    scripts/setup-nomos-circuits.sh v0.3.1 "$TMPDIR/nomos-circuits"
    cp -a "$TMPDIR/nomos-circuits"/. testing-framework/assets/stack/kzgrs_test_params/
</code></pre>
<h3 id="when-are-assets-needed"><a class="header" href="#when-are-assets-needed">When Are Assets Needed?</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Runner</th><th>When Required</th></tr></thead><tbody>
<tr><td><strong>Local</strong></td><td>Always (for DA workloads)</td></tr>
<tr><td><strong>Compose</strong></td><td>During image build (baked into <code>NOMOS_TESTNET_IMAGE</code>)</td></tr>
<tr><td><strong>K8s</strong></td><td>During image build + deployed to cluster via hostPath volume</td></tr>
</tbody></table>
</div>
<p><strong>Error without assets:</strong></p>
<pre><code>Error: missing KZG parameters at testing-framework/assets/stack/kzgrs_test_params/kzgrs_test_params
</code></pre>
<p>If you see this error, the file <code>kzgrs_test_params</code> is missing from the directory. Use <code>scripts/run-examples.sh</code> or <code>scripts/setup-nomos-circuits.sh</code> to fetch it.</p>
<h2 id="logging-and-observability"><a class="header" href="#logging-and-observability">Logging and Observability</a></h2>
<h3 id="node-logging-vs-framework-logging"><a class="header" href="#node-logging-vs-framework-logging">Node Logging vs Framework Logging</a></h3>
<p><strong>Critical distinction:</strong> Node logs and framework logs use different configuration mechanisms.</p>
<div class="table-wrapper"><table><thead><tr><th>Component</th><th>Controlled By</th><th>Purpose</th></tr></thead><tbody>
<tr><td><strong>Framework binaries</strong> (<code>cargo run -p runner-examples --bin local_runner</code>)</td><td><code>RUST_LOG</code></td><td>Runner orchestration, deployment logs</td></tr>
<tr><td><strong>Node processes</strong> (validators, executors spawned by runner)</td><td><code>NOMOS_LOG_LEVEL</code>, <code>NOMOS_LOG_FILTER</code>, <code>NOMOS_LOG_DIR</code></td><td>Consensus, DA, mempool, network logs</td></tr>
</tbody></table>
</div>
<p><strong>Common mistake:</strong> Setting <code>RUST_LOG=debug</code> only increases verbosity of the runner binary itself. Node logs remain at their default level unless you also set <code>NOMOS_LOG_LEVEL=debug</code>.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-bash"># This only makes the RUNNER verbose, not the nodes:
RUST_LOG=debug cargo run -p runner-examples --bin local_runner

# This makes the NODES verbose:
NOMOS_LOG_LEVEL=debug cargo run -p runner-examples --bin local_runner

# Both verbose (typically not needed):
RUST_LOG=debug NOMOS_LOG_LEVEL=debug cargo run -p runner-examples --bin local_runner
</code></pre>
<h3 id="logging-environment-variables"><a class="header" href="#logging-environment-variables">Logging Environment Variables</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Default</th><th>Effect</th></tr></thead><tbody>
<tr><td><code>NOMOS_LOG_DIR</code></td><td>None (console only)</td><td>Directory for per-node log files. If unset, logs go to stdout/stderr.</td></tr>
<tr><td><code>NOMOS_LOG_LEVEL</code></td><td><code>info</code></td><td>Global log level: <code>error</code>, <code>warn</code>, <code>info</code>, <code>debug</code>, <code>trace</code></td></tr>
<tr><td><code>NOMOS_LOG_FILTER</code></td><td>None</td><td>Fine-grained target filtering (e.g., <code>consensus=trace,da=debug</code>)</td></tr>
<tr><td><code>NOMOS_TESTS_TRACING</code></td><td><code>false</code></td><td>Enable tracing subscriber for local runner file logging</td></tr>
<tr><td><code>NOMOS_OTLP_ENDPOINT</code></td><td>None</td><td>OTLP trace endpoint (optional, disables OTLP noise if unset)</td></tr>
<tr><td><code>NOMOS_OTLP_METRICS_ENDPOINT</code></td><td>None</td><td>OTLP metrics endpoint (optional)</td></tr>
</tbody></table>
</div>
<p><strong>Example:</strong> Full debug logging to files:</p>
<pre><code class="language-bash">NOMOS_TESTS_TRACING=true \
NOMOS_LOG_DIR=/tmp/test-logs \
NOMOS_LOG_LEVEL=debug \
NOMOS_LOG_FILTER="nomos_consensus=trace,nomos_da_sampling=debug" \
POL_PROOF_DEV_MODE=true \
cargo run -p runner-examples --bin local_runner
</code></pre>
<h3 id="per-node-log-files"><a class="header" href="#per-node-log-files">Per-Node Log Files</a></h3>
<p>When <code>NOMOS_LOG_DIR</code> is set, each node writes logs to separate files:</p>
<p><strong>File naming pattern:</strong></p>
<ul>
<li><strong>Validators</strong>: Prefix <code>nomos-node-0</code>, <code>nomos-node-1</code>, etc. (may include timestamp suffix)</li>
<li><strong>Executors</strong>: Prefix <code>nomos-executor-0</code>, <code>nomos-executor-1</code>, etc. (may include timestamp suffix)</li>
</ul>
<p><strong>Local runner caveat:</strong> By default, the local runner writes logs to temporary directories in the working directory. These are automatically cleaned up after tests complete. To preserve logs, you MUST set both <code>NOMOS_TESTS_TRACING=true</code> AND <code>NOMOS_LOG_DIR=/path/to/logs</code>.</p>
<h3 id="filter-target-names"><a class="header" href="#filter-target-names">Filter Target Names</a></h3>
<p>Common target prefixes for <code>NOMOS_LOG_FILTER</code>:</p>
<div class="table-wrapper"><table><thead><tr><th>Target Prefix</th><th>Subsystem</th></tr></thead><tbody>
<tr><td><code>nomos_consensus</code></td><td>Consensus (Cryptarchia)</td></tr>
<tr><td><code>nomos_da_sampling</code></td><td>DA sampling service</td></tr>
<tr><td><code>nomos_da_dispersal</code></td><td>DA dispersal service</td></tr>
<tr><td><code>nomos_da_verifier</code></td><td>DA verification</td></tr>
<tr><td><code>nomos_mempool</code></td><td>Transaction mempool</td></tr>
<tr><td><code>nomos_blend</code></td><td>Mix network/privacy layer</td></tr>
<tr><td><code>chain_network</code></td><td>P2P networking</td></tr>
<tr><td><code>chain_leader</code></td><td>Leader election</td></tr>
</tbody></table>
</div>
<p><strong>Example filter:</strong></p>
<pre><code class="language-bash">NOMOS_LOG_FILTER="nomos_consensus=trace,nomos_da_sampling=debug,chain_network=info"
</code></pre>
<h3 id="accessing-logs-per-runner"><a class="header" href="#accessing-logs-per-runner">Accessing Logs Per Runner</a></h3>
<h4 id="local-runner"><a class="header" href="#local-runner">Local Runner</a></h4>
<p><strong>Default (temporary directories, auto-cleanup):</strong></p>
<pre><code class="language-bash">POL_PROOF_DEV_MODE=true cargo run -p runner-examples --bin local_runner
# Logs written to temporary directories in working directory
# Automatically cleaned up after test completes
</code></pre>
<p><strong>Persistent file output:</strong></p>
<pre><code class="language-bash">NOMOS_TESTS_TRACING=true \
NOMOS_LOG_DIR=/tmp/local-logs \
POL_PROOF_DEV_MODE=true \
cargo run -p runner-examples --bin local_runner

# After test completes:
ls /tmp/local-logs/
# Files with prefix: nomos-node-0*, nomos-node-1*, nomos-executor-0*
# May include timestamps in filename
</code></pre>
<p><strong>Both flags required:</strong> You MUST set both <code>NOMOS_TESTS_TRACING=true</code> (enables tracing file sink) AND <code>NOMOS_LOG_DIR</code> (specifies directory) to get persistent logs.</p>
<h4 id="compose-runner"><a class="header" href="#compose-runner">Compose Runner</a></h4>
<p><strong>Via Docker logs (default, recommended):</strong></p>
<pre><code class="language-bash"># List containers (note the UUID prefix in names)
docker ps --filter "name=nomos-compose-"

# Stream logs from specific container
docker logs -f &lt;container-id-or-name&gt;

# Or use name pattern matching:
docker logs -f $(docker ps --filter "name=nomos-compose-.*-validator-0" -q | head -1)
</code></pre>
<p><strong>Via file collection (advanced):</strong></p>
<p>Setting <code>NOMOS_LOG_DIR</code> writes files <strong>inside the container</strong>. To access them, you must either:</p>
<ol>
<li><strong>Copy files out after the run:</strong></li>
</ol>
<pre><code class="language-bash">NOMOS_LOG_DIR=/logs \
NOMOS_TESTNET_IMAGE=nomos-testnet:local \
POL_PROOF_DEV_MODE=true \
cargo run -p runner-examples --bin compose_runner

# After test, copy files from containers:
docker ps --filter "name=nomos-compose-"
docker cp &lt;container-id&gt;:/logs/nomos-node-0* /tmp/
</code></pre>
<ol start="2">
<li><strong>Mount a host volume</strong> (requires modifying compose template):</li>
</ol>
<pre><code class="language-yaml">volumes:
  - /tmp/host-logs:/logs  # Add to docker-compose.yml.tera
</code></pre>
<p><strong>Recommendation:</strong> Use <code>docker logs</code> by default. File collection inside containers is complex and rarely needed.</p>
<p><strong>Keep containers for debugging:</strong></p>
<pre><code class="language-bash">COMPOSE_RUNNER_PRESERVE=1 \
NOMOS_TESTNET_IMAGE=nomos-testnet:local \
cargo run -p runner-examples --bin compose_runner
# Containers remain running after test—inspect with docker logs or docker exec
</code></pre>
<p><strong>Note:</strong> Container names follow pattern <code>nomos-compose-{uuid}-validator-{index}-1</code> where <code>{uuid}</code> changes per run.</p>
<h4 id="k8s-runner"><a class="header" href="#k8s-runner">K8s Runner</a></h4>
<p><strong>Via kubectl logs (use label selectors):</strong></p>
<pre><code class="language-bash"># List pods
kubectl get pods

# Stream logs using label selectors (recommended)
kubectl logs -l app=nomos-validator -f
kubectl logs -l app=nomos-executor -f

# Stream logs from specific pod
kubectl logs -f nomos-validator-0

# Previous logs from crashed pods
kubectl logs --previous -l app=nomos-validator
</code></pre>
<p><strong>Download logs for offline analysis:</strong></p>
<pre><code class="language-bash"># Using label selectors
kubectl logs -l app=nomos-validator --tail=1000 &gt; all-validators.log
kubectl logs -l app=nomos-executor --tail=1000 &gt; all-executors.log

# Specific pods
kubectl logs nomos-validator-0 &gt; validator-0.log
kubectl logs nomos-executor-1 &gt; executor-1.log
</code></pre>
<p><strong>Specify namespace (if not using default):</strong></p>
<pre><code class="language-bash">kubectl logs -n my-namespace -l app=nomos-validator -f
</code></pre>
<h3 id="otlp-and-telemetry"><a class="header" href="#otlp-and-telemetry">OTLP and Telemetry</a></h3>
<p><strong>OTLP exporters are optional.</strong> If you see errors about unreachable OTLP endpoints, it's safe to ignore them unless you're actively collecting traces/metrics.</p>
<p><strong>To enable OTLP:</strong></p>
<pre><code class="language-bash">NOMOS_OTLP_ENDPOINT=http://localhost:4317 \
NOMOS_OTLP_METRICS_ENDPOINT=http://localhost:4318 \
cargo run -p runner-examples --bin local_runner
</code></pre>
<p><strong>To silence OTLP errors:</strong> Simply leave these variables unset (the default).</p>
<h3 id="observability-prometheus-and-node-apis"><a class="header" href="#observability-prometheus-and-node-apis">Observability: Prometheus and Node APIs</a></h3>
<p>Runners expose metrics and node HTTP endpoints for expectation code and debugging:</p>
<p><strong>Prometheus (Compose only):</strong></p>
<ul>
<li>Default: <code>http://localhost:9090</code></li>
<li>Override: <code>TEST_FRAMEWORK_PROMETHEUS_PORT=9091</code></li>
<li>Access from expectations: <code>ctx.telemetry().prometheus_endpoint()</code></li>
</ul>
<p><strong>Node APIs:</strong></p>
<ul>
<li>Access from expectations: <code>ctx.node_clients().validators().get(0)</code></li>
<li>Endpoints: consensus info, network info, DA membership, etc.</li>
<li>See <code>testing-framework/core/src/nodes/api_client.rs</code> for available methods</li>
</ul>
<pre><code class="language-mermaid">flowchart TD
    Expose[Runner exposes endpoints/ports] --&gt; Collect[Runtime collects block/health signals]
    Collect --&gt; Consume[Expectations consume signals&lt;br/&gt;decide pass/fail]
    Consume --&gt; Inspect[Operators inspect logs/metrics&lt;br/&gt;when failures arise]
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="part-iii--developer-reference"><a class="header" href="#part-iii--developer-reference">Part III — Developer Reference</a></h1>
<p>Deep dives for contributors who extend the framework, evolve its abstractions,
or maintain the crate set.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scenario-model-developer-level"><a class="header" href="#scenario-model-developer-level">Scenario Model (Developer Level)</a></h1>
<p>The scenario model defines clear, composable responsibilities:</p>
<ul>
<li><strong>Topology</strong>: a declarative description of the cluster—how many nodes, their
roles, and the broad network and data-availability characteristics. It
represents the intended shape of the system under test.</li>
<li><strong>Scenario</strong>: a plan combining topology, workloads, expectations, and a run
window. Building a scenario validates prerequisites (like seeded wallets) and
ensures the run lasts long enough to observe meaningful block progression.</li>
<li><strong>Workloads</strong>: asynchronous tasks that generate traffic or conditions. They
use shared context to interact with the deployed cluster and may bundle
default expectations.</li>
<li><strong>Expectations</strong>: post-run assertions. They can capture baselines before
workloads start and evaluate success once activity stops.</li>
<li><strong>Runtime</strong>: coordinates workloads and expectations for the configured
duration, enforces cooldowns when control actions occur, and ensures cleanup
so runs do not leak resources.</li>
</ul>
<p>Developers extending the model should keep these boundaries strict: topology
describes, scenarios assemble, deployers provision, runners orchestrate,
workloads drive, and expectations judge outcomes. For guidance on adding new
capabilities, see <a href="extending.html">Extending the Framework</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="extending-the-framework"><a class="header" href="#extending-the-framework">Extending the Framework</a></h1>
<h2 id="adding-a-workload"><a class="header" href="#adding-a-workload">Adding a workload</a></h2>
<ol>
<li>Implement <code>testing_framework_core::scenario::Workload</code>:
<ul>
<li>Provide a name and any bundled expectations.</li>
<li>In <code>init</code>, derive inputs from <code>GeneratedTopology</code> and <code>RunMetrics</code>; fail
fast if prerequisites are missing (e.g., wallet data, node addresses).</li>
<li>In <code>start</code>, drive async traffic using the <code>RunContext</code> clients.</li>
</ul>
</li>
<li>Expose the workload from a module under <code>testing-framework/workflows</code> and
consider adding a DSL helper for ergonomic wiring.</li>
</ol>
<h2 id="adding-an-expectation"><a class="header" href="#adding-an-expectation">Adding an expectation</a></h2>
<ol>
<li>Implement <code>testing_framework_core::scenario::Expectation</code>:
<ul>
<li>Use <code>start_capture</code> to snapshot baseline metrics.</li>
<li>Use <code>evaluate</code> to assert outcomes after workloads finish; return all errors
so the runner can aggregate them.</li>
</ul>
</li>
<li>Export it from <code>testing-framework/workflows</code> if it is reusable.</li>
</ol>
<h2 id="adding-a-runner"><a class="header" href="#adding-a-runner">Adding a runner</a></h2>
<ol>
<li>Implement <code>testing_framework_core::scenario::Deployer</code> for your backend.
<ul>
<li>Produce a <code>RunContext</code> with <code>NodeClients</code>, metrics endpoints, and optional
<code>NodeControlHandle</code>.</li>
<li>Guard cleanup with <code>CleanupGuard</code> to reclaim resources even on failures.</li>
</ul>
</li>
<li>Mirror the readiness and block-feed probes used by the existing runners so
workloads can rely on consistent signals.</li>
</ol>
<h2 id="adding-topology-helpers"><a class="header" href="#adding-topology-helpers">Adding topology helpers</a></h2>
<ul>
<li>Extend <code>testing_framework_core::topology::TopologyBuilder</code> with new layouts or
configuration presets (e.g., specialized DA parameters). Keep defaults safe:
ensure at least one participant and clamp dispersal factors as the current
helpers do.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="example-new-workload--expectation-rust"><a class="header" href="#example-new-workload--expectation-rust">Example: New Workload &amp; Expectation (Rust)</a></h1>
<p>A minimal, end-to-end illustration of adding a custom workload and matching
expectation. This shows the shape of the traits and where to plug into the
framework; expand the logic to fit your real test.</p>
<h2 id="workload-simple-reachability-probe"><a class="header" href="#workload-simple-reachability-probe">Workload: simple reachability probe</a></h2>
<p>Key ideas:</p>
<ul>
<li><strong>name</strong>: identifies the workload in logs.</li>
<li><strong>expectations</strong>: workloads can bundle defaults so callers don’t forget checks.</li>
<li><strong>init</strong>: derive inputs from the generated topology (e.g., pick a target node).</li>
<li><strong>start</strong>: drive async activity using the shared <code>RunContext</code>.</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::sync::Arc;
use async_trait::async_trait;
use testing_framework_core::scenario::{
    DynError, Expectation, RunContext, RunMetrics, Workload,
};
use testing_framework_core::topology::GeneratedTopology;

pub struct ReachabilityWorkload {
    target_idx: usize,
    bundled: Vec&lt;Box&lt;dyn Expectation&gt;&gt;,
}

impl ReachabilityWorkload {
    pub fn new(target_idx: usize) -&gt; Self {
        Self {
            target_idx,
            bundled: vec![Box::new(ReachabilityExpectation::new(target_idx))],
        }
    }
}

#[async_trait]
impl Workload for ReachabilityWorkload {
    fn name(&amp;self) -&gt; &amp;'static str {
        "reachability_workload"
    }

    fn expectations(&amp;self) -&gt; Vec&lt;Box&lt;dyn Expectation&gt;&gt; {
        self.bundled.clone()
    }

    fn init(
        &amp;mut self,
        topology: &amp;GeneratedTopology,
        _metrics: &amp;RunMetrics,
    ) -&gt; Result&lt;(), DynError&gt; {
        if topology.validators().get(self.target_idx).is_none() {
            return Err("no validator at requested index".into());
        }
        Ok(())
    }

    async fn start(&amp;self, ctx: &amp;RunContext) -&gt; Result&lt;(), DynError&gt; {
        let client = ctx
            .clients()
            .validators()
            .get(self.target_idx)
            .ok_or("missing target client")?;

        // Pseudo-action: issue a lightweight RPC to prove reachability.
        client.health_check().await.map_err(|e| e.into())
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="expectation-confirm-the-target-stayed-reachable"><a class="header" href="#expectation-confirm-the-target-stayed-reachable">Expectation: confirm the target stayed reachable</a></h2>
<p>Key ideas:</p>
<ul>
<li><strong>start_capture</strong>: snapshot baseline if needed (not used here).</li>
<li><strong>evaluate</strong>: assert the condition after workloads finish.</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use async_trait::async_trait;
use testing_framework_core::scenario::{DynError, Expectation, RunContext};

pub struct ReachabilityExpectation {
    target_idx: usize,
}

impl ReachabilityExpectation {
    pub fn new(target_idx: usize) -&gt; Self {
        Self { target_idx }
    }
}

#[async_trait]
impl Expectation for ReachabilityExpectation {
    fn name(&amp;self) -&gt; &amp;str {
        "target_reachable"
    }

    async fn evaluate(&amp;mut self, ctx: &amp;RunContext) -&gt; Result&lt;(), DynError&gt; {
        let client = ctx
            .clients()
            .validators()
            .get(self.target_idx)
            .ok_or("missing target client")?;

        client.health_check().await.map_err(|e| {
            format!("target became unreachable during run: {e}").into()
        })
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="how-to-wire-it"><a class="header" href="#how-to-wire-it">How to wire it</a></h2>
<ul>
<li>Build your scenario as usual and call <code>.with_workload(ReachabilityWorkload::new(0))</code>.</li>
<li>The bundled expectation is attached automatically; you can add more with
<code>.with_expectation(...)</code> if needed.</li>
<li>Keep the logic minimal and fast for smoke tests; grow it into richer probes
for deeper scenarios.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="internal-crate-reference"><a class="header" href="#internal-crate-reference">Internal Crate Reference</a></h1>
<p>High-level roles of the crates that make up the framework:</p>
<ul>
<li>
<p><strong>Configs</strong> (<code>testing-framework/configs/</code>): Prepares reusable configuration primitives for nodes, networking, tracing, data availability, and wallets, shared by all scenarios and runners. Includes topology generation and circuit asset resolution.</p>
</li>
<li>
<p><strong>Core scenario orchestration</strong> (<code>testing-framework/core/</code>): Houses the topology and scenario model, runtime coordination, node clients, and readiness/health probes. Defines <code>Deployer</code> and <code>Runner</code> traits, <code>ScenarioBuilder</code>, and <code>RunContext</code>.</p>
</li>
<li>
<p><strong>Workflows</strong> (<code>testing-framework/workflows/</code>): Packages workloads (transaction, DA, chaos) and expectations (consensus liveness) into reusable building blocks. Offers fluent DSL extensions (<code>ScenarioBuilderExt</code>, <code>ChaosBuilderExt</code>).</p>
</li>
<li>
<p><strong>Runners</strong> (<code>testing-framework/runners/{local,compose,k8s}/</code>): Implements deployment backends (local host, Docker Compose, Kubernetes) that all consume the same scenario plan. Each provides a <code>Deployer</code> implementation (<code>LocalDeployer</code>, <code>ComposeDeployer</code>, <code>K8sDeployer</code>).</p>
</li>
<li>
<p><strong>Runner Examples</strong> (<code>examples/runner-examples</code>): Runnable binaries demonstrating framework usage and serving as living documentation. These are the <strong>primary entry point</strong> for running scenarios (<code>local_runner.rs</code>, <code>compose_runner.rs</code>, <code>k8s_runner.rs</code>).</p>
</li>
</ul>
<h2 id="where-to-add-new-capabilities"><a class="header" href="#where-to-add-new-capabilities">Where to Add New Capabilities</a></h2>
<div class="table-wrapper"><table><thead><tr><th>What You're Adding</th><th>Where It Goes</th><th>Examples</th></tr></thead><tbody>
<tr><td><strong>Node config parameter</strong></td><td><code>testing-framework/configs/src/topology/configs/</code></td><td>Slot duration, log levels, DA params</td></tr>
<tr><td><strong>Topology feature</strong></td><td><code>testing-framework/core/src/topology/</code></td><td>New network layouts, node roles</td></tr>
<tr><td><strong>Scenario capability</strong></td><td><code>testing-framework/core/src/scenario/</code></td><td>New capabilities, context methods</td></tr>
<tr><td><strong>Workload</strong></td><td><code>testing-framework/workflows/src/workloads/</code></td><td>New traffic generators</td></tr>
<tr><td><strong>Expectation</strong></td><td><code>testing-framework/workflows/src/expectations/</code></td><td>New success criteria</td></tr>
<tr><td><strong>Builder API</strong></td><td><code>testing-framework/workflows/src/builder/</code></td><td>DSL extensions, fluent methods</td></tr>
<tr><td><strong>Deployer</strong></td><td><code>testing-framework/runners/</code></td><td>New deployment backends</td></tr>
<tr><td><strong>Example scenario</strong></td><td><code>examples/src/bin/</code></td><td>Demonstration binaries</td></tr>
</tbody></table>
</div>
<h2 id="extension-workflow"><a class="header" href="#extension-workflow">Extension Workflow</a></h2>
<h3 id="adding-a-new-workload"><a class="header" href="#adding-a-new-workload">Adding a New Workload</a></h3>
<ol>
<li>
<p><strong>Define the workload</strong> in <code>testing-framework/workflows/src/workloads/your_workload.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use async_trait::async_trait;
use testing_framework_core::scenario::{Workload, RunContext, DynError};

pub struct YourWorkload {
    // config fields
}

#[async_trait]
impl Workload for YourWorkload {
    fn name(&amp;self) -&gt; &amp;'static str { "your_workload" }
    async fn start(&amp;self, ctx: &amp;RunContext) -&gt; Result&lt;(), DynError&gt; {
        // implementation
        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Add builder extension</strong> in <code>testing-framework/workflows/src/builder/mod.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait ScenarioBuilderExt {
    fn your_workload(self) -&gt; YourWorkloadBuilder;
}
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Use in examples</strong> in <code>examples/src/bin/your_scenario.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut plan = ScenarioBuilder::topology_with(|t| {
        t.network_star()
            .validators(3)
            .executors(0)
    })
    .your_workload_with(|w| {  // Your new DSL method with closure
        w.some_config()
    })
    .build();
<span class="boring">}</span></code></pre></pre>
</li>
</ol>
<h3 id="adding-a-new-expectation"><a class="header" href="#adding-a-new-expectation">Adding a New Expectation</a></h3>
<ol>
<li>
<p><strong>Define the expectation</strong> in <code>testing-framework/workflows/src/expectations/your_expectation.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use async_trait::async_trait;
use testing_framework_core::scenario::{Expectation, RunContext, DynError};

pub struct YourExpectation {
    // config fields
}

#[async_trait]
impl Expectation for YourExpectation {
    fn name(&amp;self) -&gt; &amp;str { "your_expectation" }
    async fn evaluate(&amp;mut self, ctx: &amp;RunContext) -&gt; Result&lt;(), DynError&gt; {
        // implementation
        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Add builder extension</strong> in <code>testing-framework/workflows/src/builder/mod.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait ScenarioBuilderExt {
    fn expect_your_condition(self) -&gt; Self;
}
<span class="boring">}</span></code></pre></pre>
</li>
</ol>
<h3 id="adding-a-new-deployer"><a class="header" href="#adding-a-new-deployer">Adding a New Deployer</a></h3>
<ol>
<li>
<p><strong>Implement <code>Deployer</code> trait</strong> in <code>testing-framework/runners/your_runner/src/deployer.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use async_trait::async_trait;
use testing_framework_core::scenario::{Deployer, Runner, Scenario};

pub struct YourDeployer;

#[async_trait]
impl Deployer for YourDeployer {
    type Error = YourError;
    
    async fn deploy(&amp;self, scenario: &amp;Scenario) -&gt; Result&lt;Runner, Self::Error&gt; {
        // Provision infrastructure
        // Wait for readiness
        // Return Runner
    }
}
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Provide cleanup</strong> and handle node control if supported.</p>
</li>
<li>
<p><strong>Add example</strong> in <code>examples/src/bin/your_runner.rs</code>.</p>
</li>
</ol>
<p>For detailed examples, see <a href="extending.html">Extending the Framework</a> and <a href="custom-workload-example.html">Custom Workload Example</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="part-iv--appendix"><a class="header" href="#part-iv--appendix">Part IV — Appendix</a></h1>
<p>Quick-reference material and supporting guidance to keep scenarios discoverable,
debuggable, and consistent.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="builder-api-quick-reference"><a class="header" href="#builder-api-quick-reference">Builder API Quick Reference</a></h1>
<p>Quick reference for the scenario builder DSL. All methods are chainable.</p>
<h2 id="imports"><a class="header" href="#imports">Imports</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use testing_framework_core::scenario::{Deployer, ScenarioBuilder};
use testing_framework_runner_local::LocalDeployer;
use testing_framework_runner_compose::ComposeDeployer;
use testing_framework_runner_k8s::K8sDeployer;
use testing_framework_workflows::{ScenarioBuilderExt, ChaosBuilderExt};
use std::time::Duration;
<span class="boring">}</span></code></pre></pre>
<h2 id="topology"><a class="header" href="#topology">Topology</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>ScenarioBuilder::topology_with(|t| {
        t.network_star()      // Star topology (all connect to seed node)
            .validators(3)    // Number of validator nodes
            .executors(2)     // Number of executor nodes
    })                        // Finish topology configuration
<span class="boring">}</span></code></pre></pre>
<h2 id="wallets"><a class="header" href="#wallets">Wallets</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>.wallets(50)                 // Seed 50 funded wallet accounts
<span class="boring">}</span></code></pre></pre>
<h2 id="transaction-workload-1"><a class="header" href="#transaction-workload-1">Transaction Workload</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>.transactions_with(|txs| {
    txs.rate(5)              // 5 transactions per block
        .users(20)           // Use 20 of the seeded wallets
})                           // Finish transaction workload config
<span class="boring">}</span></code></pre></pre>
<h2 id="da-workload"><a class="header" href="#da-workload">DA Workload</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>.da_with(|da| {
    da.channel_rate(1)       // 1 channel operation per block
        .blob_rate(2)        // 2 blob dispersals per block
})                           // Finish DA workload config
<span class="boring">}</span></code></pre></pre>
<h2 id="chaos-workload-requires-enable_node_control"><a class="header" href="#chaos-workload-requires-enable_node_control">Chaos Workload (Requires <code>enable_node_control()</code>)</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>.enable_node_control()       // Enable node control capability
.chaos_with(|c| {
    c.restart()              // Random restart chaos
        .min_delay(Duration::from_secs(30))     // Min time between restarts
        .max_delay(Duration::from_secs(60))     // Max time between restarts
        .target_cooldown(Duration::from_secs(45))  // Cooldown after restart
        .apply()             // Required for chaos configuration
})
<span class="boring">}</span></code></pre></pre>
<h2 id="expectations-1"><a class="header" href="#expectations-1">Expectations</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>.expect_consensus_liveness() // Assert blocks are produced continuously
<span class="boring">}</span></code></pre></pre>
<h2 id="run-duration"><a class="header" href="#run-duration">Run Duration</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>.with_run_duration(Duration::from_secs(120))  // Run for 120 seconds
<span class="boring">}</span></code></pre></pre>
<h2 id="build"><a class="header" href="#build">Build</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>.build()                     // Construct the final Scenario
<span class="boring">}</span></code></pre></pre>
<h2 id="deployers-1"><a class="header" href="#deployers-1">Deployers</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Local processes
let deployer = LocalDeployer::default();

// Docker Compose
let deployer = ComposeDeployer::default();

// Kubernetes
let deployer = K8sDeployer::default();
<span class="boring">}</span></code></pre></pre>
<h2 id="execution"><a class="header" href="#execution">Execution</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let runner = deployer.deploy(&amp;plan).await?;
let _handle = runner.run(&amp;mut plan).await?;
<span class="boring">}</span></code></pre></pre>
<h2 id="complete-example"><a class="header" href="#complete-example">Complete Example</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use testing_framework_core::scenario::{Deployer, ScenarioBuilder};
use testing_framework_runner_local::LocalDeployer;
use testing_framework_workflows::ScenarioBuilderExt;
use std::time::Duration;

async fn run_test() -&gt; Result&lt;(), Box&lt;dyn std::error::Error + Send + Sync&gt;&gt; {
    let mut plan = ScenarioBuilder::topology_with(|t| {
            t.network_star()
                .validators(3)
                .executors(2)
        })
        .wallets(50)
        .transactions_with(|txs| {
            txs.rate(5)                     // 5 transactions per block
                .users(20)
        })
        .da_with(|da| {
            da.channel_rate(1)             // 1 channel operation per block
                .blob_rate(2)              // 2 blob dispersals per block
        })
        .expect_consensus_liveness()
        .with_run_duration(Duration::from_secs(90))
        .build();

    let deployer = LocalDeployer::default();
    let runner = deployer.deploy(&amp;plan).await?;
    let _handle = runner.run(&amp;mut plan).await?;
    
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="troubleshooting-scenarios"><a class="header" href="#troubleshooting-scenarios">Troubleshooting Scenarios</a></h1>
<p><strong>Prerequisites for All Runners:</strong></p>
<ul>
<li><strong><code>versions.env</code> file</strong> at repository root (required by helper scripts)</li>
<li><strong><code>POL_PROOF_DEV_MODE=true</code></strong> MUST be set for all runners (host, compose, k8s) to avoid expensive Groth16 proof generation that causes timeouts</li>
<li><strong>KZG circuit assets</strong> must be present at <code>testing-framework/assets/stack/kzgrs_test_params/kzgrs_test_params</code> (note the repeated filename) for DA workloads</li>
</ul>
<p><strong>Recommended:</strong> Use <code>scripts/run-examples.sh</code> which handles all setup automatically.</p>
<h2 id="quick-symptom-guide"><a class="header" href="#quick-symptom-guide">Quick Symptom Guide</a></h2>
<p>Common symptoms and likely causes:</p>
<ul>
<li><strong>No or slow block progression</strong>: missing <code>POL_PROOF_DEV_MODE=true</code>, missing KZG circuit assets (<code>/kzgrs_test_params/kzgrs_test_params</code> file) for DA workloads, too-short run window, port conflicts, or resource exhaustion—set required env vars, verify assets exist, extend duration, check node logs for startup errors.</li>
<li><strong>Transactions not included</strong>: unfunded or misconfigured wallets (check <code>.wallets(N)</code> vs <code>.users(M)</code>), transaction rate exceeding block capacity, or rates exceeding block production speed—reduce rate, increase wallet count, verify wallet setup in logs.</li>
<li><strong>Chaos stalls the run</strong>: chaos (node control) only works with ComposeDeployer; host runner (LocalDeployer) and K8sDeployer don't support it (won't "stall", just can't execute chaos workloads). With compose, aggressive restart cadence can prevent consensus recovery—widen restart intervals.</li>
<li><strong>Observability gaps</strong>: metrics or logs unreachable because ports clash or services are not exposed—adjust observability ports and confirm runner wiring.</li>
<li><strong>Flaky behavior across runs</strong>: mixing chaos with functional smoke tests or inconsistent topology between environments—separate deterministic and chaos scenarios and standardize topology presets.</li>
</ul>
<h2 id="where-to-find-logs"><a class="header" href="#where-to-find-logs">Where to Find Logs</a></h2>
<h3 id="log-location-quick-reference"><a class="header" href="#log-location-quick-reference">Log Location Quick Reference</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Runner</th><th>Default Output</th><th>With <code>NOMOS_LOG_DIR</code> + Flags</th><th>Access Command</th></tr></thead><tbody>
<tr><td><strong>Host</strong> (local)</td><td>Temporary directories (cleaned up)</td><td>Per-node files with prefix <code>nomos-node-{index}</code> (requires <code>NOMOS_TESTS_TRACING=true</code>)</td><td><code>cat $NOMOS_LOG_DIR/nomos-node-0*</code></td></tr>
<tr><td><strong>Compose</strong></td><td>Docker container stdout/stderr</td><td>Per-node files inside containers (if path is mounted)</td><td><code>docker ps</code> then <code>docker logs &lt;container-id&gt;</code></td></tr>
<tr><td><strong>K8s</strong></td><td>Pod stdout/stderr</td><td>Per-node files inside pods (if path is mounted)</td><td><code>kubectl logs -l app=nomos-validator</code></td></tr>
</tbody></table>
</div>
<p><strong>Important Notes:</strong></p>
<ul>
<li><strong>Host runner</strong> (local processes): Logs go to system temporary directories (NOT in working directory) by default and are automatically cleaned up after tests. To persist logs, you MUST set both <code>NOMOS_TESTS_TRACING=true</code> AND <code>NOMOS_LOG_DIR=/path/to/logs</code>.</li>
<li><strong>Compose/K8s</strong>: Per-node log files only exist inside containers/pods if <code>NOMOS_LOG_DIR</code> is set AND the path is writable inside the container/pod. By default, rely on <code>docker logs</code> or <code>kubectl logs</code>.</li>
<li><strong>File naming</strong>: Log files use prefix <code>nomos-node-{index}*</code> or <code>nomos-executor-{index}*</code> with timestamps, e.g., <code>nomos-node-0.2024-12-01T10-30-45.log</code> (NOT just <code>.log</code> suffix).</li>
<li><strong>Container names</strong>: Compose containers include project UUID, e.g., <code>nomos-compose-&lt;uuid&gt;-validator-0-1</code> where <code>&lt;uuid&gt;</code> is randomly generated per run</li>
</ul>
<h3 id="accessing-node-logs-by-runner"><a class="header" href="#accessing-node-logs-by-runner">Accessing Node Logs by Runner</a></h3>
<h4 id="local-runner-1"><a class="header" href="#local-runner-1">Local Runner</a></h4>
<p><strong>Console output (default):</strong></p>
<pre><code class="language-bash">POL_PROOF_DEV_MODE=true cargo run -p runner-examples --bin local_runner 2&gt;&amp;1 | tee test.log
</code></pre>
<p><strong>Persistent file output:</strong></p>
<pre><code class="language-bash">NOMOS_TESTS_TRACING=true \
NOMOS_LOG_DIR=/tmp/debug-logs \
NOMOS_LOG_LEVEL=debug \
POL_PROOF_DEV_MODE=true \
cargo run -p runner-examples --bin local_runner

# Inspect logs (note: filenames include timestamps):
ls /tmp/debug-logs/
# Example: nomos-node-0.2024-12-01T10-30-45.log
tail -f /tmp/debug-logs/nomos-node-0*  # Use wildcard to match timestamp
</code></pre>
<h4 id="compose-runner-1"><a class="header" href="#compose-runner-1">Compose Runner</a></h4>
<p><strong>Stream live logs:</strong></p>
<pre><code class="language-bash"># List running containers (note the UUID prefix in names)
docker ps --filter "name=nomos-compose-"

# Find your container ID or name from the list, then:
docker logs -f &lt;container-id&gt;

# Or filter by name pattern:
docker logs -f $(docker ps --filter "name=nomos-compose-.*-validator-0" -q | head -1)

# Show last 100 lines
docker logs --tail 100 &lt;container-id&gt;
</code></pre>
<p><strong>Keep containers for post-mortem debugging:</strong></p>
<pre><code class="language-bash">COMPOSE_RUNNER_PRESERVE=1 \
NOMOS_TESTNET_IMAGE=nomos-testnet:local \
POL_PROOF_DEV_MODE=true \
cargo run -p runner-examples --bin compose_runner

# OR: Use run-examples.sh (handles setup automatically)
COMPOSE_RUNNER_PRESERVE=1 scripts/run-examples.sh -t 60 -v 1 -e 1 compose

# After test failure, containers remain running:
docker ps --filter "name=nomos-compose-"
docker exec -it &lt;container-id&gt; /bin/sh
docker logs &lt;container-id&gt; &gt; debug.log
</code></pre>
<p><strong>Note:</strong> Container names follow the pattern <code>nomos-compose-{uuid}-validator-{index}-1</code> or <code>nomos-compose-{uuid}-executor-{index}-1</code>, where <code>{uuid}</code> is randomly generated per run.</p>
<h4 id="k8s-runner-1"><a class="header" href="#k8s-runner-1">K8s Runner</a></h4>
<p><strong>Important:</strong> Always verify your namespace and use label selectors instead of assuming pod names.</p>
<p><strong>Stream pod logs (use label selectors):</strong></p>
<pre><code class="language-bash"># Check your namespace first
kubectl config view --minify | grep namespace

# All validator pods (add -n &lt;namespace&gt; if not using default)
kubectl logs -l app=nomos-validator -f

# All executor pods
kubectl logs -l app=nomos-executor -f

# Specific pod by name (find exact name first)
kubectl get pods -l app=nomos-validator  # Find the exact pod name
kubectl logs -f &lt;actual-pod-name&gt;        # Then use it

# With explicit namespace
kubectl logs -n my-namespace -l app=nomos-validator -f
</code></pre>
<p><strong>Download logs from crashed pods:</strong></p>
<pre><code class="language-bash"># Previous logs from crashed pod
kubectl get pods -l app=nomos-validator  # Find crashed pod name first
kubectl logs --previous &lt;actual-pod-name&gt; &gt; crashed-validator.log

# Or use label selector for all crashed validators
for pod in $(kubectl get pods -l app=nomos-validator -o name); do
  kubectl logs --previous $pod &gt; $(basename $pod)-previous.log 2&gt;&amp;1
done
</code></pre>
<p><strong>Access logs from all pods:</strong></p>
<pre><code class="language-bash"># All pods in current namespace
for pod in $(kubectl get pods -o name); do
  echo "=== $pod ==="
  kubectl logs $pod
done &gt; all-logs.txt

# Or use label selectors (recommended)
kubectl logs -l app=nomos-validator --tail=500 &gt; validators.log
kubectl logs -l app=nomos-executor --tail=500 &gt; executors.log

# With explicit namespace
kubectl logs -n my-namespace -l app=nomos-validator --tail=500 &gt; validators.log
</code></pre>
<h2 id="debugging-workflow"><a class="header" href="#debugging-workflow">Debugging Workflow</a></h2>
<p>When a test fails, follow this sequence:</p>
<h3 id="1-check-framework-output"><a class="header" href="#1-check-framework-output">1. Check Framework Output</a></h3>
<p>Start with the test harness output—did expectations fail? Was there a deployment error?</p>
<p><strong>Look for:</strong></p>
<ul>
<li>Expectation failure messages</li>
<li>Timeout errors</li>
<li>Deployment/readiness failures</li>
</ul>
<h3 id="2-verify-node-readiness"><a class="header" href="#2-verify-node-readiness">2. Verify Node Readiness</a></h3>
<p>Ensure all nodes started successfully and became ready before workloads began.</p>
<p><strong>Commands:</strong></p>
<pre><code class="language-bash"># Local: check process list
ps aux | grep nomos

# Compose: check container status (note UUID in names)
docker ps -a --filter "name=nomos-compose-"

# K8s: check pod status (use label selectors, add -n &lt;namespace&gt; if needed)
kubectl get pods -l app=nomos-validator
kubectl get pods -l app=nomos-executor
kubectl describe pod &lt;actual-pod-name&gt;  # Get name from above first
</code></pre>
<h3 id="3-inspect-node-logs"><a class="header" href="#3-inspect-node-logs">3. Inspect Node Logs</a></h3>
<p>Focus on the first node that exhibited problems or the node with the highest index (often the last to start).</p>
<p><strong>Common error patterns:</strong></p>
<ul>
<li>"ERROR: versions.env missing" → missing required <code>versions.env</code> file at repository root</li>
<li>"Failed to bind address" → port conflict</li>
<li>"Connection refused" → peer not ready or network issue</li>
<li>"Proof verification failed" or "Proof generation timeout" → missing <code>POL_PROOF_DEV_MODE=true</code> (REQUIRED for all runners)</li>
<li>"Failed to load KZG parameters" or "Circuit file not found" → missing KZG circuit assets at <code>testing-framework/assets/stack/kzgrs_test_params/</code></li>
<li>"Insufficient funds" → wallet seeding issue (increase <code>.wallets(N)</code> or reduce <code>.users(M)</code>)</li>
</ul>
<h3 id="4-check-log-levels"><a class="header" href="#4-check-log-levels">4. Check Log Levels</a></h3>
<p>If logs are too sparse, increase verbosity:</p>
<pre><code class="language-bash">NOMOS_LOG_LEVEL=debug \
NOMOS_LOG_FILTER="nomos_consensus=trace,nomos_da_sampling=debug" \
cargo run -p runner-examples --bin local_runner
</code></pre>
<h3 id="5-verify-observability-endpoints"><a class="header" href="#5-verify-observability-endpoints">5. Verify Observability Endpoints</a></h3>
<p>If expectations report observability issues:</p>
<p><strong>Prometheus (Compose):</strong></p>
<pre><code class="language-bash">curl http://localhost:9090/-/healthy
</code></pre>
<p><strong>Node HTTP APIs:</strong></p>
<pre><code class="language-bash">curl http://localhost:18080/consensus/info  # Adjust port per node
</code></pre>
<h3 id="6-compare-with-known-good-scenario"><a class="header" href="#6-compare-with-known-good-scenario">6. Compare with Known-Good Scenario</a></h3>
<p>Run a minimal baseline test (e.g., 2 validators, consensus liveness only). If it passes, the issue is in your workload or topology configuration.</p>
<h2 id="common-error-messages"><a class="header" href="#common-error-messages">Common Error Messages</a></h2>
<h3 id="consensus-liveness-expectation-failed"><a class="header" href="#consensus-liveness-expectation-failed">"Consensus liveness expectation failed"</a></h3>
<ul>
<li><strong>Cause</strong>: Not enough blocks produced during the run window, missing
<code>POL_PROOF_DEV_MODE=true</code> (causes slow proof generation), or missing KZG
assets for DA workloads.</li>
<li><strong>Fix</strong>:
<ol>
<li>Verify <code>POL_PROOF_DEV_MODE=true</code> is set (REQUIRED for all runners).</li>
<li>Verify KZG assets exist at
<code>testing-framework/assets/stack/kzgrs_test_params/</code> (for DA workloads).</li>
<li>Extend <code>with_run_duration()</code> to allow more blocks.</li>
<li>Check node logs for proof generation or DA errors.</li>
<li>Reduce transaction/DA rate if nodes are overwhelmed.</li>
</ol>
</li>
</ul>
<h3 id="wallet-seeding-failed"><a class="header" href="#wallet-seeding-failed">"Wallet seeding failed"</a></h3>
<ul>
<li><strong>Cause</strong>: Topology doesn't have enough funded wallets for the workload.</li>
<li><strong>Fix</strong>: Increase <code>.wallets(N)</code> count or reduce <code>.users(M)</code> in the transaction
workload (ensure N ≥ M).</li>
</ul>
<h3 id="node-control-not-available"><a class="header" href="#node-control-not-available">"Node control not available"</a></h3>
<ul>
<li><strong>Cause</strong>: Runner doesn't support node control (only ComposeDeployer does), or
<code>enable_node_control()</code> wasn't called.</li>
<li><strong>Fix</strong>:
<ol>
<li>Use ComposeDeployer for chaos tests (LocalDeployer and K8sDeployer don't
support node control).</li>
<li>Ensure <code>.enable_node_control()</code> is called in the scenario before <code>.chaos()</code>.</li>
</ol>
</li>
</ul>
<h3 id="readiness-timeout"><a class="header" href="#readiness-timeout">"Readiness timeout"</a></h3>
<ul>
<li><strong>Cause</strong>: Nodes didn't become responsive within expected time (often due to
missing prerequisites).</li>
<li><strong>Fix</strong>:
<ol>
<li><strong>Verify <code>POL_PROOF_DEV_MODE=true</code> is set</strong> (REQUIRED for all runners—without
it, proof generation is too slow).</li>
<li>Check node logs for startup errors (port conflicts, missing assets).</li>
<li>Verify network connectivity between nodes.</li>
<li>For DA workloads, ensure KZG circuit assets are present.</li>
</ol>
</li>
</ul>
<h3 id="error-versionsenv-missing"><a class="header" href="#error-versionsenv-missing">"ERROR: versions.env missing"</a></h3>
<ul>
<li><strong>Cause</strong>: Helper scripts (<code>run-examples.sh</code>, <code>build-bundle.sh</code>, <code>setup-circuits-stack.sh</code>) require <code>versions.env</code> file at repository root.</li>
<li><strong>Fix</strong>: Ensure you're running from the repository root directory. The <code>versions.env</code> file should already exist and contains:
<pre><code>VERSION=v0.3.1
NOMOS_NODE_REV=d2dd5a5084e1daef4032562c77d41de5e4d495f8
NOMOS_BUNDLE_VERSION=v4
</code></pre>
If the file is missing, restore it from version control or create it with the above content.</li>
</ul>
<h3 id="port-already-in-use"><a class="header" href="#port-already-in-use">"Port already in use"</a></h3>
<ul>
<li><strong>Cause</strong>: Previous test didn't clean up, or another process holds the port.</li>
<li><strong>Fix</strong>: Kill orphaned processes (<code>pkill nomos-node</code>), wait for Docker cleanup
(<code>docker compose down</code>), or restart Docker.</li>
</ul>
<h3 id="image-not-found-nomos-testnetlocal"><a class="header" href="#image-not-found-nomos-testnetlocal">"Image not found: nomos-testnet:local"</a></h3>
<ul>
<li><strong>Cause</strong>: Docker image not built for Compose/K8s runners, or KZG assets not
baked into the image.</li>
<li><strong>Fix (recommended)</strong>: Use run-examples.sh which handles everything:
<pre><code class="language-bash">scripts/run-examples.sh -t 60 -v 1 -e 1 compose
</code></pre>
</li>
<li><strong>Fix (manual)</strong>:
<ol>
<li>Build bundle: <code>scripts/build-bundle.sh --platform linux</code></li>
<li>Set bundle path: <code>export NOMOS_BINARIES_TAR=.tmp/nomos-binaries-linux-v0.3.1.tar.gz</code></li>
<li>Build image: <code>testing-framework/assets/stack/scripts/build_test_image.sh</code></li>
</ol>
</li>
</ul>
<h3 id="failed-to-load-kzg-parameters-or-circuit-file-not-found"><a class="header" href="#failed-to-load-kzg-parameters-or-circuit-file-not-found">"Failed to load KZG parameters" or "Circuit file not found"</a></h3>
<ul>
<li><strong>Cause</strong>: DA workload requires KZG circuit assets. The file <code>testing-framework/assets/stack/kzgrs_test_params/kzgrs_test_params</code> (note repeated filename) must exist. Inside containers, it's at <code>/kzgrs_test_params/kzgrs_test_params</code>.</li>
<li><strong>Fix (recommended)</strong>: Use run-examples.sh which handles setup:
<pre><code class="language-bash">scripts/run-examples.sh -t 60 -v 1 -e 1 &lt;mode&gt;
</code></pre>
</li>
<li><strong>Fix (manual)</strong>:
<ol>
<li>Fetch assets: <code>scripts/setup-nomos-circuits.sh v0.3.1 /tmp/nomos-circuits</code></li>
<li>Copy to expected path: <code>cp -r /tmp/nomos-circuits/* testing-framework/assets/stack/kzgrs_test_params/</code></li>
<li>Verify file exists: <code>ls -lh testing-framework/assets/stack/kzgrs_test_params/kzgrs_test_params</code></li>
<li>For Compose/K8s: rebuild image with assets baked in</li>
</ol>
</li>
</ul>
<p>For detailed logging configuration and observability setup, see <a href="operations.html">Operations</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="faq"><a class="header" href="#faq">FAQ</a></h1>
<p><strong>Why block-oriented timing?</strong><br />
Slots advance at a fixed rate (NTP-synchronized, 2s by default), so reasoning
about blocks and consensus intervals keeps assertions aligned with protocol
behavior rather than arbitrary wall-clock durations.</p>
<p><strong>Can I reuse the same scenario across runners?</strong><br />
Yes. The plan stays the same; swap runners (local, compose, k8s) to target
different environments.</p>
<p><strong>When should I enable chaos workloads?</strong><br />
Only when testing resilience or operational recovery; keep functional smoke
tests deterministic.</p>
<p><strong>How long should runs be?</strong><br />
The framework enforces a minimum of <strong>2× slot duration</strong> (4 seconds with default 2s slots), but practical recommendations:</p>
<ul>
<li><strong>Smoke tests</strong>: 30s minimum (~14 blocks with default 2s slots, 0.9 coefficient)</li>
<li><strong>Transaction workloads</strong>: 60s+ (~27 blocks) to observe inclusion patterns</li>
<li><strong>DA workloads</strong>: 90s+ (~40 blocks) to account for dispersal and sampling</li>
<li><strong>Chaos tests</strong>: 120s+ (~54 blocks) to allow recovery after restarts</li>
</ul>
<p>Very short runs (&lt; 30s) risk false confidence—one or two lucky blocks don't prove liveness.</p>
<p><strong>Do I always need seeded wallets?</strong><br />
Only for transaction scenarios. Data-availability or pure chaos scenarios may
not require them, but liveness checks still need validators producing blocks.</p>
<p><strong>What if expectations fail but workloads “look fine”?</strong><br />
Trust expectations first—they capture the intended success criteria. Use the
observability signals and runner logs to pinpoint why the system missed the
target.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="glossary"><a class="header" href="#glossary">Glossary</a></h1>
<ul>
<li><strong>Validator</strong>: node role responsible for participating in consensus and block
production.</li>
<li><strong>Executor</strong>: a validator node with the DA dispersal service enabled. Executors
can submit transactions and disperse blob data to the DA network, in addition
to performing all validator functions.</li>
<li><strong>DA (Data Availability)</strong>: subsystem ensuring blobs or channel data are
published and retrievable for validation.</li>
<li><strong>Deployer</strong>: component that provisions infrastructure (spawns processes,
creates containers, or launches pods), waits for readiness, and returns a
Runner. Examples: LocalDeployer, ComposeDeployer, K8sDeployer.</li>
<li><strong>Runner</strong>: component returned by deployers that orchestrates scenario
execution—starts workloads, observes signals, evaluates expectations, and
triggers cleanup.</li>
<li><strong>Workload</strong>: traffic or behavior generator that exercises the system during a
scenario run.</li>
<li><strong>Expectation</strong>: post-run assertion that judges whether the system met the
intended success criteria.</li>
<li><strong>Topology</strong>: declarative description of the cluster shape, roles, and
high-level parameters for a scenario.</li>
<li><strong>Scenario</strong>: immutable plan combining topology, workloads, expectations, and
run duration.</li>
<li><strong>Blockfeed</strong>: stream of block observations used for liveness or inclusion
signals during a run.</li>
<li><strong>Control capability</strong>: the ability for a runner to start, stop, or restart
nodes, used by chaos workloads.</li>
<li><strong>Slot duration</strong>: time interval between consensus rounds in Cryptarchia. Blocks
are produced at multiples of the slot duration based on lottery outcomes.</li>
<li><strong>Block cadence</strong>: observed rate of block production in a live network, measured
in blocks per second or seconds per block.</li>
<li><strong>Cooldown</strong>: waiting period after a chaos action (e.g., node restart) before
triggering the next action, allowing the system to stabilize.</li>
<li><strong>Run window</strong>: total duration a scenario executes, specified via
<code>with_run_duration()</code>. Framework auto-extends to at least 2× slot duration.</li>
<li><strong>Readiness probe</strong>: health check performed by runners to ensure nodes are
reachable and responsive before starting workloads. Prevents false negatives
from premature traffic.</li>
<li><strong>Liveness</strong>: property that the system continues making progress (producing
blocks) under specified conditions. Contrasts with safety/correctness which
verifies that state transitions are accurate.</li>
<li><strong>State assertion</strong>: expectation that verifies specific values in the system
state (e.g., wallet balances, UTXO sets) rather than just progress signals.
Also called "correctness expectations."</li>
<li><strong>Mantle transaction</strong>: transaction type in Nomos that can contain UTXO transfers
(LedgerTx) and operations (Op), including channel data (ChannelBlob).</li>
<li><strong>Channel</strong>: logical grouping for DA blobs; each blob belongs to a channel and
references a parent blob in the same channel, creating a chain of related data.</li>
<li><strong>POL_PROOF_DEV_MODE</strong>: environment variable that disables expensive Groth16 zero-knowledge
proof generation for leader election. <strong>Required for all runners</strong> (local, compose, k8s)
for practical testing—without it, proof generation causes timeouts. Should never be
used in production environments.</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="theme/mermaid-init.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
